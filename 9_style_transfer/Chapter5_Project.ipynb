{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_S4vgMlbIUw"
   },
   "source": [
    "## Chapter 5. Project \n",
    "\n",
    "## Neural Painter - an application to colourise sharp pencil sketches \n",
    "\n",
    "We have now successfully learnt the fundamentals of Neural Style Transfer and also implemented an end to end style transfer pipeline. \n",
    "\n",
    "In this unit we will apply what we learnt to build one more cool project.  We are going to apply style transfer concepts to build a **Neural Painter** which takes an uncoloured image and colours it based on the the style image.  Let's dive in to it . \n",
    "\n",
    "This project will take in a sharp pencil sktech of any object as content image. I highlight the word sharp because we need the outlines to be well preserved in the outputs too. The style image will be the painted version of a similar object. It need not be the same object though.If you want to be a bit more adventurous try with diffent objects to get colourful outputs. \n",
    "\n",
    "We are going to read images from url this time, which means you can provide url of any image of choice from web (preferablely google images search). This also means, you can directly run this on **Google Colab** as well (https://colab.research.google.com/notebooks/welcome.ipynb) \n",
    "\n",
    "What will you learn from this project :\n",
    "- Building the complete style transfer pipeline\n",
    "- Adapting style transfer to binary or gray scale images. So you can apply it to any kind of images (graycale or RGB)\n",
    "- Controlling the tuning parameters to control the nature of the output \n",
    "- Utilizing the pretrained networks like VGG-19 to extract features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87y9RqXZFgEJ"
   },
   "source": [
    "## Import libs\n",
    "First as usual we import all the necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "i8o4R34Y8VaB",
    "outputId": "f328f898-9f73-40aa-90b4-5a414adfbde6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version >=  1.12.0\n",
      "Keras version >=  2.2.4\n",
      "Numpy version >=  1.17.2\n",
      "Matplotlib version >=  2.2.2\n",
      "OpenCV version >=  4.1.0\n",
      "Scikit-image version >=  0.15.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "import  tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage import io\n",
    "print(\"Tensorflow version >= \" , tf.__version__)\n",
    "print(\"Keras version >= \" , keras.__version__)\n",
    "print(\"Numpy version >= \" , np.__version__)\n",
    "print(\"Matplotlib version >= \" , matplotlib.__version__)\n",
    "print(\"OpenCV version >= \" , cv2.__version__)\n",
    "print(\"Scikit-image version >= \" , skimage.__version__)\n",
    "\n",
    "def mkdir(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BylrwMyKbMDd"
   },
   "source": [
    "### USER INPUTS\n",
    "Here comes the user input section. \n",
    "\n",
    "Content image : Search for pencil sketch or border image of any object of your choice from google images and provide the url. \n",
    "\n",
    "Style image : Search for a painted version of the object and provide the url \n",
    "\n",
    "Tune the parameters to play with the results. I have kept the content weight high so as to make sure my content image features are preserved till the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76y4B3YBJE8B"
   },
   "outputs": [],
   "source": [
    "content_img_url = 'https://i1.wp.com/flowernifty.com/wp-content/uploads/images/drawn-daisy-flower-petal-pencil-and-in-color-drawn-daisy-daisy-drawing-flower.jpg'\n",
    "style_img_url = 'http://2.bp.blogspot.com/-KETjBj3RQZw/USraH2mQJtI/AAAAAAAAB00/SLn5ny3v6AE/s1600/1329+Pocketful+of+Sunshine.jpg'\n",
    "\n",
    "#### Height of generated image. \n",
    "#### This will be used to calculate the width of generated images based on Aspect Ratio of original image\n",
    "img_nrows = 300\n",
    "\n",
    "############## Folders to store content , style and output images \n",
    "content_image_path = 'images/content_imgs_1/'\n",
    "style_image_path = 'images/style_imgs_1/'\n",
    "out_folder = 'ST_results_1' \n",
    "mkdir(out_folder)\n",
    "mkdir(content_image_path)\n",
    "mkdir(style_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSBfeZAmFrH0"
   },
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCO5J3JQGvAA"
   },
   "source": [
    "### Read image from URL  : \n",
    "To make our code user friendly, let's read images from URL this time. Also since our content image will be binary image mostly, we will have a condition to convert it to RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AE49OyPIzvB"
   },
   "outputs": [],
   "source": [
    "def get_ImgfrmURL(url):\n",
    "  img = io.imread(url)\n",
    "  if img.ndim == 2: #If the image gray scale , convert it to BGR \n",
    "    img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "  else:\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "  return img\n",
    "\n",
    "def imshow(img):\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcjK3835b4QO"
   },
   "source": [
    "##  <span style=\"color:RED\"> TASK : </span> Preprocessing and Deprocessing of the images to suit VGG-19 requirements\n",
    "Here we implement the same VGG based preprocessing functions as implemented in the previous chapter. \n",
    "\n",
    "Please refer to Chapter 4  for more details on preprocessing and deprocessing . \n",
    "\n",
    "Create the following functions taking reference from Chapter 4 and the function hints below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNEvdOzrBl5T"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications import vgg19\n",
    "\n",
    "def prepare_img_array(img_path , target_size = (224,224)):\n",
    "    \"\"\" \n",
    "    TODO :\n",
    "    Create the function in such a way that it :\n",
    "    - loads the image and scale it to givcen target size\n",
    "    - converts it to array\n",
    "    - expands dimensions along axis 0  \n",
    "    \n",
    "    Hint : \n",
    "    from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load image from img_path and scale to specified target_size\n",
    "    img_arr = \n",
    "    \n",
    "    # Convert the image to array\n",
    "    img_arr = \n",
    "    \n",
    "    # Expand dimensions along axis = 0 \n",
    "    img_arr = \n",
    "    return img_arr\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    TODO :\n",
    "    Create the function to process the image\n",
    "    Hint :  vgg19.preprocess_input()\n",
    "    \n",
    "    \"\"\"\n",
    "    # Prepare a copy of the img to avoid the original array being mutated\n",
    "    img_copy = np.copy(img)\n",
    "    \n",
    "    # Preprocess image \n",
    "    pp_img = \n",
    "    \n",
    "    return pp_img\n",
    "\n",
    "def deprocess_image(img_preprocessed , img_nrows , img_ncols):\n",
    "    \"\"\"\n",
    "    Deprocess the image preprocessed image\n",
    "    \n",
    "    The steps to be implemented are : \n",
    "    - Remove zero-center by mean pixel based on VGG standard values \n",
    "    - Convert to RGB \n",
    "    - Clip the image values to be between 0 t0 255\n",
    "    \n",
    "    Hint :\n",
    "\n",
    "    - The VGG standard values are : \n",
    "      Red_MEAN = 123.68 \n",
    "      Green_MEAN = 116.779\n",
    "      Blue_MEAN = 103.939 \n",
    "      \n",
    "    \n",
    "    \"\"\"\n",
    "    # Prepare a copy of the img_preprocessed to avoid the original array being mutated    \n",
    "    img = np.copy(img_preprocessed)\n",
    "    \n",
    "    # Prepare conditions to handle channels_first, channels_last format \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        img = img.reshape((3, img_nrows, img_ncols))\n",
    "        img = img.transpose((1, 2, 0))\n",
    "    else:\n",
    "        img = img.reshape((img_nrows, img_ncols, 3))\n",
    "    print(img.shape)\n",
    "    \n",
    "    # 'BGR'->'RGB' another simple way. cv2.cvtColor(img,cv2.COLOR_BGR2RGB) can be used as well \n",
    "    img = \n",
    "    \n",
    "    # Remove zero-center by mean pixel based on VGG standard values\n",
    "    img[:, :, 0] += \n",
    "    img[:, :, 1] += \n",
    "    img[:, :, 2] += \n",
    "    \n",
    "    # Clip the values to be betwene 0 to 255 and convert it to unit8\n",
    "    img = \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "kljK8BkLbfFZ",
    "outputId": "af51fd01-4377-4e04-d4c2-80b29a35782f"
   },
   "outputs": [],
   "source": [
    "url_content_img = get_ImgfrmURL(content_img_url)\n",
    "print(\"CONTENT IMAGE (Pencil sketch/border image) \")\n",
    "imshow(url_content_img)\n",
    "print(\"STYLE IMAGE\")\n",
    "url_style_img = get_ImgfrmURL(style_img_url)\n",
    "imshow(url_style_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the content and style images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xvk4nZnKBex0"
   },
   "outputs": [],
   "source": [
    "content_image_path = content_image_path + '/content1.jpg'\n",
    "style_image_path = style_image_path + '/style1.jpg'\n",
    "\n",
    "cv2.imwrite(content_image_path , url_content_img)\n",
    "cv2.imwrite(style_image_path , url_style_img)\n",
    "\n",
    "############## Generated (output) Image Dimensions\n",
    "width, height = load_img(content_image_path).size\n",
    "img_ncols = int(width * img_nrows / height)#Width of generated image\n",
    "generated_imsize = (img_nrows , img_ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwjYj410QpBz"
   },
   "source": [
    "### Test processed and deprocessed images \n",
    "Just a visualisation of processed and deprocessed images. Though the processed images look weird, they are just the mean centred images according to VGG processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jpp2JXMjBqOd",
    "outputId": "07035ca3-147f-4ade-bce5-e70e651a694e"
   },
   "outputs": [],
   "source": [
    "# get tensor representations of our images\n",
    "content_image = prepare_img_array(content_image_path , generated_imsize)\n",
    "style_image = prepare_img_array(style_image_path , generated_imsize)\n",
    "\n",
    "content_image_pp = preprocess_image(content_image)\n",
    "style_image_pp = preprocess_image(style_image)\n",
    "\n",
    "content_image_dp = deprocess_image(content_image_pp, img_nrows , img_ncols)\n",
    "style_image_dp = deprocess_image(style_image_pp, img_nrows , img_ncols)\n",
    "\n",
    "orig_image = content_image\n",
    "\n",
    "print(\"Orig Image\")\n",
    "plt.imshow(orig_image[0].astype('uint8'))\n",
    "plt.show()\n",
    "print(\"Preprocessed Image\")\n",
    "plt.imshow(content_image_pp[0].astype('uint8'))\n",
    "plt.show()\n",
    "print(\"Deprocessed Image\")\n",
    "plt.imshow(content_image_dp.astype('uint8'))\n",
    "plt.show()\n",
    "print(\"Orig Image\")\n",
    "plt.imshow(style_image[0].astype('uint8'))\n",
    "plt.show()\n",
    "print(\"Preprocessed Image\")\n",
    "plt.imshow(style_image_pp[0].astype('uint8'))\n",
    "plt.show()\n",
    "print(\"Deprocessed Image\")\n",
    "plt.imshow(style_image_dp.astype('uint8'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzfXNv9NcS2v"
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "Now let us build the loss functions : Content loss, Gram matrix, Style loss and Total variation loss. \n",
    "You can refer Chapters 3 and 4 for more information\n",
    "\n",
    "###   <span style=\"color:RED\"> TASK : </span> Content Loss (L<sub>content</sub>)\n",
    "Implement Content Loss \n",
    "\n",
    "**Hint :** \n",
    "\n",
    " Use K.sum . K.square for implementation\n",
    "\n",
    " The content loss can be formulated as follows : \n",
    "\n",
    " ![](https://bitbucket.org/ga_learning/style_transfer/raw/48db127941e04f2f66cdfd6a3ccaca9ed888d598/markdown_images/Content_loss.png)\n",
    " \n",
    " where , \n",
    "\n",
    "* **L<sub>content</sub>** is the Content Loss \n",
    "\n",
    "* **l** is the layer from which the feature maps are obtained\n",
    "\n",
    "* **i** refers to the index of each feature maps from layer l\n",
    "\n",
    "* **j** refers to each element in the flattened feature matrix of size h x w\n",
    "\n",
    "* **F** is the feature map from the Content image (C) obtained at layer l\n",
    "\n",
    "* **P** is the feature map from the Generated image (G) obtained at layer l \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lk4q4HtEL5RJ"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def content_loss(base, combination):\n",
    "    content_loss = \n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-kMd6KhcXmY"
   },
   "source": [
    "###   <span style=\"color:RED\"> TASK : </span>  Gram Matrix\n",
    "\n",
    "Implement Gram matrix calculation function\n",
    "\n",
    "**Hint :** \n",
    "\n",
    "Use K.dot , K.transpose\n",
    "\n",
    "Gram matrix equation : \n",
    "\n",
    "![](https://bitbucket.org/ga_learning/style_transfer/raw/48db127941e04f2f66cdfd6a3ccaca9ed888d598/markdown_images/Gram_matrix.png)\n",
    "\n",
    "where, \n",
    "\n",
    "* **G<sub>ij</sub>** is the Gram matrix \n",
    "\n",
    "* **i** refers to the number of feature maps at layer l\n",
    "\n",
    "* **k** refers to each element in the flattened feature matrix of size h x w\n",
    "\n",
    "* **F** is the feature map from the Content image (C) obtained at selected layer l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QT-8ExPrL-4G"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = \n",
    "    return gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   <span style=\"color:RED\"> TASK : </span>  Style Loss (L<sub>style</sub>)\n",
    "\n",
    "Implement Style loss calculation function\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Use the previous Gram Matrix function, K.sum, K.square\n",
    "\n",
    "Style Loss equation : \n",
    "\n",
    "![](https://bitbucket.org/ga_learning/style_transfer/raw/48db127941e04f2f66cdfd6a3ccaca9ed888d598/markdown_images/Style_loss.png)\n",
    "\n",
    "where, \n",
    "\n",
    "* **L<sub>style</sub>** is the Style Loss \n",
    "\n",
    "* **l** is the layer from which the feature maps are obtained\n",
    "\n",
    "* **i** refers to the index of each feature maps from layer l\n",
    "\n",
    "* **j** refers to each element in the flattened feature matrix of size h x w\n",
    "\n",
    "* **A<sub>ij</sub>** is the Gram matrix for the Input Image features\n",
    "\n",
    "* **G<sub>ij</sub>** is the Gram matrix for the Generated Image features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    \n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "\n",
    "    style_loss = K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFi-sKEvcbLZ"
   },
   "source": [
    "###   <span style=\"color:RED\"> TASK : </span>   Total Variation Loss (L<sub>tv</sub>)\n",
    "\n",
    "The total variance loss can be represented as **L<sub>tv</sub>**\n",
    "\n",
    "**L<sub>tv</sub> = Elementwise_Sum( (G<sub>x</sub><sup>2</sup> + G<sub>y</sub><sup>2</sup>) <sup>1.25</sup> )**\n",
    "\n",
    "where,\n",
    "\n",
    "* G<sub>x</sub> = Gradient of Image along x-axis\n",
    "* G<sub>y</sub> = Gradient of Image along y-axis\n",
    "\n",
    "**Hint:**\n",
    "Use K.square , K.sum , K.pow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEseJVr0MD4d"
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        a = K.square(\n",
    "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
    "        b = K.square(\n",
    "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(\n",
    "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "        b = K.square(\n",
    "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FT7BcUQDcHqs"
   },
   "source": [
    "##  <span style=\"color:RED\"> TASK : </span>  Create a composite tensor to hold Content Image , Style Image and Generated (Combined) Image\n",
    "\n",
    "The preprocessed images created so far are in array format. \n",
    "Create a single tensor to hold the :\n",
    "- preprocessed content image (content_image_pp)\n",
    "- preprocessed style image (style_image_pp)\n",
    "- Generated (output) image (combination_image)\n",
    "\n",
    "Note that the combination image is still not generated yet and hence we need to define a placeholder for the same. \n",
    "\n",
    "Hints : \n",
    "- Use K.variable to convert numpy array to Tensor, \n",
    "- Use K.placeholder to create placeholder\n",
    "- Use K.concatenate to combine the tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjggYFfZLgw_"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Convert images from array to tensor\n",
    "content_image_pp_tensor = \n",
    "style_image_pp_tensor = \n",
    "\n",
    "# Preparing the tensor placeholder for holding the generated image\n",
    "if K.image_data_format() == 'channels_first':  # (1,3,img_nrows, img_ncols)\n",
    "    combination_image_tensor = \n",
    "else: # (1,img_nrows, img_ncols, 3)\n",
    "    combination_image_tensor = \n",
    "\n",
    "# Concatenate content_image_pp_tensor, style_image_pp_tensor, combination_image_tensor \n",
    "# into a single Keras tensor along axis = 0 \n",
    "input_tensor = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R61FNwcwcL3T"
   },
   "source": [
    "##  <span style=\"color:RED\"> TASK : </span> VGG-19 Model \n",
    "\n",
    "Load the pretrained VGG-19 model (inbuilt in Keras) and print the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "GZ-mhOz8LoAj",
    "outputId": "6b0c8227-7d60-4900-d704-6905a7acdcee"
   },
   "outputs": [],
   "source": [
    "from keras.applications import vgg19\n",
    "\n",
    "def load_VGG19(input_tensor , include_top=False, weights='imagenet'):\n",
    "    \"\"\"\n",
    "     Load the vgg 19 pretrained model \n",
    "     \n",
    "     include_top: whether to include the 3 fully-connected layers at the top of the network.\n",
    "     weights: None (random initialization) or 'imagenet' (pre-training on ImageNet)\n",
    "     input_tensor: optional Keras tensor (i.e. output of layers.Input()) to use as image input for the model\n",
    "     \n",
    "     More options at : \n",
    "     https://keras.io/applications/#extract-features-from-an-arbitrary-intermediate-layer-with-vgg19\n",
    "    \n",
    "    \"\"\"\n",
    "    model =  \n",
    "    return model \n",
    "\n",
    "include_top = False\n",
    "vgg19_model = load_VGG19(input_tensor,include_top,weights='imagenet')\n",
    "vgg19_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5SgDAsCclOh"
   },
   "source": [
    "###   <span style=\"color:RED\"> TASK : </span>  Initialize parameters for Style Transfer \n",
    "\n",
    "Set up the style transfer parameters. Tweak the values and observe the effects on the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Style transfer parameters \n",
    "content_weight = 1.0 #alpha\n",
    "style_weight = 0.5 #beta\n",
    "total_variation_weight = 0.5  #gamma\n",
    "iterations = 50 # Number of iteration to optimise the Total-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   <span style=\"color:RED\"> TASK : </span>  Calculate losses\n",
    "\n",
    "- Extract features from loaded VGG-19 model and initialise the content loss, style loss, total variation loss\n",
    "\n",
    "- Make sure you use the above defined content_weight, style_weight and total_variation weights while calculating losses\n",
    "\n",
    "- Calculate the total loss : loss = sum of content loss, style loss, total variation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6QrXJdfiMG2H",
    "outputId": "8812e5da-5aa7-4f53-cc31-a698359d5f17"
   },
   "outputs": [],
   "source": [
    "# Make a dictionary of layers of the loaded VGG-19 model with the layer names as the keys \n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in vgg19_model.layers])\n",
    "\n",
    "# Calculate Content loss\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "\n",
    "loss_content = \n",
    "\n",
    "# Calculate Style loss\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "loss_style = 0 #Initialise Style loss\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    \n",
    "    loss_style_layerwise = \n",
    "    \n",
    "    loss_style += loss_style_layerwise\n",
    "\n",
    "# Calculate Total Variation loss\n",
    "loss_tv =    \n",
    "\n",
    "# Calculate the total loss (loss)\n",
    "loss = K.variable(0.0)\n",
    "loss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   <span style=\"color:RED\"> TASK : </span>   Calculate gradients \n",
    "\n",
    "- Calculate the gradients using K.gradients\n",
    "\n",
    "Hint : \n",
    "Use K.gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Get the gradients of the generated image wrt the loss and the combination_image_tensor \n",
    "grads = K.gradients(loss, combination_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Keras function to output loss and grads during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_n_grads = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    loss_n_grads += grads\n",
    "else:\n",
    "    loss_n_grads.append(grads)\n",
    "\n",
    "# Initialise a function to output loss and gradient values \n",
    "f_outputs = K.function([combination_image_tensor], loss_n_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kTHRp-PcptF"
   },
   "source": [
    "### Evaluation function to calculate the gradient and losses  \n",
    "Let us now write our loss evaluation function. \n",
    "\n",
    "It is organised in the same way as in the project in Chapter 4 and can be used as standard for these kind of loss optimisation problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4tJOG85MKue"
   },
   "outputs": [],
   "source": [
    "def eval_loss_and_grads(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
    "    else:\n",
    "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOk95UY2crJj"
   },
   "source": [
    "## Optimising the the losses and generating the output \n",
    "\n",
    "The Evaluator() class defined above provides methods to access loss and gradients.Now we have a computation graph ready . How do we optimize it?\n",
    "There are many optimisation techniques ranging from simplest (like gradient_descent) to most sophisticated ones (like ADAM) \n",
    "\n",
    "In this case, for ease of implementation we can use limited memory BFGS, from scipy.optimize package. It helps us optimise our loss functions easily without breaking our head on the implementation part. Please have a look at https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html#scipy-optimize-fmin-l-bfgs-b for more details.\n",
    "\n",
    "Wonderful !! We now have eveything to optimise our network to colour up our uncoloured content image. Just make sure you have set all the parameters in the \"User inputs\" cell. \n",
    "\n",
    "We are all set to run the optimisation below! The results will be stored with iteration number as postfix in the output folder mentioned at input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   <span style=\"color:RED\"> TASK : </span>   Creating the style transfer pipeline\n",
    "- Create the optimisation pipeline using  `fmin_l_bfgs_b()` as optimisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4267
    },
    "colab_type": "code",
    "id": "d3x4WdtWMOtZ",
    "outputId": "88798209-6557-4104-e00e-c3ce54a66aad"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "input_image = content_image\n",
    "\n",
    "# Preprocess content image\n",
    "out_image = \n",
    "\n",
    "generated_images = []\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TODO : Use the fmin_l_bfgs_b for optimization\n",
    "    out_image, min_val, info = \n",
    "    print('Current loss value:', min_val)\n",
    "    \n",
    "    # deprocess image to get the styled output\n",
    "    deprocessed_result = \n",
    "    \n",
    "    \n",
    "    fname = out_folder + '/out_at_iteration_%d.png' % i\n",
    "    save_img(fname, deprocessed_result)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
    "    generated_images.append(deprocessed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-cJZ07qcwn0"
   },
   "source": [
    "### Output Visualisation\n",
    "Have a look at how the generated image becomes more and more realistic as we move through the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2104
    },
    "colab_type": "code",
    "id": "YWh26Iy7MT6d",
    "outputId": "e5d98f7f-582b-4274-b879-da6a350310ed"
   },
   "outputs": [],
   "source": [
    "print(\"Content Image\")\n",
    "plt.imshow(content_image_dp.astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\nStyle Image\")\n",
    "plt.imshow(style_image_dp.astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "for i,g in enumerate(generated_images):\n",
    "    if i%10 == 0 :\n",
    "        print(\"\\n\\n================= Iteration : \",i)\n",
    "        plt.imshow(g)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Content Image\")\n",
    "plt.imshow(content_image_dp.astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\nStyle Image\")\n",
    "plt.imshow(style_image_dp.astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\nColoured image\")\n",
    "plt.imshow(generated_images[-1].astype('uint8'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well Done !\n",
    "\n",
    "Congratulations ! You have now learnt the concepts needed to build your own Neural Style Transfer Pipeline successfully. With the project in Neural style transfer project in Chapter 4 we learnt how to use a pretrained network and a cascade of losses to create beautiful fusion of images. \n",
    "\n",
    "With this project we were able to demonstrate a simple yet very useful application of style transfer ie., colourisation of uncoloured images, by just changing our content image to a black and white pencil sketch image and processing it appropriately.  Now you can freely experiment it even further and build cool apps as per your imaginations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "Below are the list of some interesting papers on Style Transfer and it's applications:\n",
    "\n",
    "* A Neural Algorithm of Artistic Style\n",
    "* Neural Style Transfer: A Review\n",
    "* Deep Photo Style Transfer\n",
    "* There is a very nice collection of style transfer related papers and their implementations at : https://paperswithcode.com/task/style-transfer . Have a look at them in case you wish to learn more about implementation of advanced style transfer and it's extensions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "StyleTransferPipeline_project.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
