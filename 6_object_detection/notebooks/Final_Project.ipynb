{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Object detection\n",
    "Welcome back guys. Now this is the final step of your course module. Yes this is the application project. We will apply all the learned knowledge in here to create an interesting application based on object detection. We will use the different dataset called football detection dataset.\n",
    "\n",
    "## Tasks\n",
    "- Write code to do the following things \n",
    "   - visualize with bbox \n",
    "   - histogram of number of instances per image\n",
    "   - histogram of size of the objects \n",
    "   - histogram of aspect ratios of the objects \n",
    "- train the network for 10 epochs and calculate the mAP \n",
    "\n",
    "## Dataset.\n",
    "\n",
    "The dataset contains snapshot of images from various football matches. All the images are annotated with football using xyxy notation. There are a total of 4177 train images and 764 validation images. Detection of football will allow the video camreas to focus on the ball and the surrondings, allowing us to track the ball movement from time to time and open up new possibilities for calculating various other KPIs, improve video quality, reply analysis and better highlights in the soccer commumity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Visualize with bbox\n",
    "Before starting any machine learning problem, it is good to have proper understanding of the data. In machine vision problems, one get proper understanding of the data by looking into the images and calculating various statistics. In this process visualizing bboxes on the images is very crutial as you want to know what your model is getting trained. The bbox coordinates are shared in different formats (xxyy, xyxy, xyhw, xywh) and sometimes scaled by image height and width etc, so to understand data well, it is always advised to visualize the data first along with coordinates and this is going to be our first task.\n",
    "\n",
    "Read the train.csv file. select the image_id \"72d1210d48a2dd4e\" and plot the football labels on the image using draw_grid function available in utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from utils import draw_grid\n",
    "\n",
    "## Root location\n",
    "root = \"/mnt/nfshome1/FRACTAL/vanapalli.prakash/Desktop/data/\"\n",
    "imgid = \"72d1210d48a2dd4e\"\n",
    "\n",
    "## Read the dataframe\n",
    "train = pd.read_csv(root+\"x_train.csv\", header=None)\n",
    "print(train.shape)\n",
    "\n",
    "## Select the image bboxs \n",
    "bbox = train[train[0] == imgid][[1, 2, 3, 4]].values\n",
    "print(bbox)\n",
    "\n",
    "## select the labels \n",
    "labels = train[train[0] == imgid][5].values\n",
    "print(labels)\n",
    "\n",
    "## Read the image\n",
    "img_loc = root+\"football/\"+imgid+\".jpg\"\n",
    "img = Image.open(img_loc)\n",
    "\n",
    "## Draw bouding box on the image\n",
    "image_with_grid= draw_grid(img.copy(), bbox, labels, outline=\"red\", input_format=\"xyxy\")\n",
    "image_with_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 histogram of number of instances per image\n",
    "How many objects are there per image ? is often the most important question object detection researchers need to ask for them selves. This will let us check if the data is skewed, any outliers etc\n",
    "\n",
    "Read the train.csv file, count the instances of football for each image and use matplotlib to plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "## Root location\n",
    "root = \"/mnt/nfshome1/FRACTAL/vanapalli.prakash/Desktop/data/\"\n",
    "\n",
    "## Read the data\n",
    "train = pd.read_csv(root+\"x_train.csv\", header=None)\n",
    "\n",
    "## take counts of each ImageID\n",
    "counts = train[0].value_counts().values\n",
    "\n",
    "## plot using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(counts, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 histogram of size of the objects\n",
    "knowing the size of the objects will let us know whether to use FPN and which layers of resnet architecture to use etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "## Root location\n",
    "root = \"/mnt/nfshome1/FRACTAL/vanapalli.prakash/Desktop/data/\"\n",
    "\n",
    "## Read the data\n",
    "train = pd.read_csv(root+\"x_train.csv\", header=None)\n",
    "\n",
    "## calculate area o fthe objects \n",
    "size = ((train[3] - train[1]) * (train[4] - train[2])).values\n",
    "\n",
    "## plot using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(size, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 histogram of aspect ratios of the objects\n",
    "\n",
    "Ratios and scales we are using are usually derived from the data. We can check the aspect ratios of your objects and change these values accordingly. For doing so we first need to compute aspect ratios (h/w) of all the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "## Root location\n",
    "root = \"/mnt/nfshome1/FRACTAL/vanapalli.prakash/Desktop/data/\"\n",
    "\n",
    "## Read the data\n",
    "train = pd.read_csv(root+\"x_train.csv\", header=None)\n",
    "\n",
    "## calculate area o fthe objects (h/w)\n",
    "aspect_ratio = ( (train[4] - train[2])/ (train[3] - train[1])).values\n",
    "\n",
    "## plot using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(aspect_ratio, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 train the network for 10 epochs and calculate the mAP \n",
    "Lets use our 05_practical_101 session in the task. We need to train our football detector. for which we need the following things.\n",
    "\n",
    "- read train and val files.\n",
    "- set up the environment and start the session\n",
    "- load the data generators - we will be using resnet50 as the backend.\n",
    "- set the image_min_side as 800 and max side as 1333 and use batch_size of 2.\n",
    "- Load prediction model and training model\n",
    "- create callbacks for evaluation, schedular and snapshots\n",
    "- train for 10 epochs. since training images are ~4200 and batch_size is 2. Lets use steps per epoch as 2100( 4200/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 6) (764, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "## Root location\n",
    "root = \"/mnt/nfshome1/FRACTAL/vanapalli.prakash/Desktop/data/\"\n",
    "\n",
    "## Read train and val files\n",
    "train = pd.read_csv(root+\"x_train.csv\", header=None)\n",
    "val = pd.read_csv(root+\"x_val.csv\", header=None)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000067f3419a12fe.jpg</td>\n",
       "      <td>126</td>\n",
       "      <td>526</td>\n",
       "      <td>232</td>\n",
       "      <td>634</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000c4f95a9d5a54.jpg</td>\n",
       "      <td>601</td>\n",
       "      <td>802</td>\n",
       "      <td>743</td>\n",
       "      <td>942</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0005c78e30d1e388.jpg</td>\n",
       "      <td>194</td>\n",
       "      <td>794</td>\n",
       "      <td>280</td>\n",
       "      <td>871</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006d33feb26fe48.jpg</td>\n",
       "      <td>699</td>\n",
       "      <td>102</td>\n",
       "      <td>732</td>\n",
       "      <td>133</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0009590f2c11c93d.jpg</td>\n",
       "      <td>428</td>\n",
       "      <td>594</td>\n",
       "      <td>465</td>\n",
       "      <td>629</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0    1    2    3    4         5\n",
       "0  000067f3419a12fe.jpg  126  526  232  634  football\n",
       "1  0000c4f95a9d5a54.jpg  601  802  743  942  football\n",
       "2  0005c78e30d1e388.jpg  194  794  280  871  football\n",
       "3  0006d33feb26fe48.jpg  699  102  732  133  football\n",
       "4  0009590f2c11c93d.jpg  428  594  465  629  football"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "\n",
    "## using gpu0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "## start the session\n",
    "def get_session():\n",
    "    \"\"\" Construct a modified tf session.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.5', '1.', '2.']\n"
     ]
    }
   ],
   "source": [
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.config import read_config_file\n",
    "from keras_retinanet.utils.transform import random_transform_generator\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "\n",
    "\n",
    "## Read the config file which contains all the parameters\n",
    "config = read_config_file(root+\"config.ini\")\n",
    "print(config[\"anchor_parameters\"][\"ratios\"].split(\" \"))\n",
    "\n",
    "backbone = models.backbone(\"resnet50\")\n",
    "\n",
    "batch_size = 2 \n",
    "image_min_side = 800\n",
    "image_max_side = 1333 \n",
    "preprocess_image = backbone.preprocess_image\n",
    "\n",
    "\n",
    "## start the genertor\n",
    "transform_generator = random_transform_generator(\n",
    "            min_rotation=-0.1,\n",
    "            max_rotation=0.1,\n",
    "            min_translation=(-0.1, -0.1),\n",
    "            max_translation=(0.1, 0.1),\n",
    "            min_shear=-0.1,\n",
    "            max_shear=0.1,\n",
    "            min_scaling=(0.9, 0.9),\n",
    "            max_scaling=(1.1, 1.1),\n",
    "            flip_x_chance=0.5,\n",
    "            flip_y_chance=0.5,\n",
    ")\n",
    "\n",
    "train_ann_loc = root+ \"x_train.csv\"\n",
    "classes_loc = root+ \"class_id.csv\"\n",
    "val_ann_loc = root+ \"x_val.csv\"\n",
    "train_generator = CSVGenerator(train_ann_loc,\n",
    "                               classes_loc,\n",
    "                               base_dir = root+\"football/\",\n",
    "                               transform_generator=transform_generator,\n",
    "                               batch_size = batch_size, \n",
    "                               image_min_side = image_min_side, \n",
    "                               image_max_side = image_max_side, \n",
    "                               preprocess_image = preprocess_image,\n",
    "                               config = config)\n",
    "\n",
    "validation_generator = CSVGenerator(val_ann_loc,\n",
    "                                    classes_loc,\n",
    "                                    base_dir = root+\"football/\",\n",
    "                                    batch_size = batch_size,\n",
    "                                    image_min_side = image_min_side, \n",
    "                                    image_max_side = image_max_side, \n",
    "                                    preprocess_image = preprocess_image,\n",
    "                                    config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/nfshome1/FRACTAL/vanapalli.prakash/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfshome1/FRACTAL/vanapalli.prakash/miniconda3/envs/keras/lib/python3.6/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 9) vs (720, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "/mnt/nfshome1/FRACTAL/vanapalli.prakash/miniconda3/envs/keras/lib/python3.6/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((9,) vs (720,)).\n",
      "  weight_values[i].shape))\n"
     ]
    }
   ],
   "source": [
    "from keras_retinanet.utils.config import parse_anchor_parameters\n",
    "from keras_retinanet.models.retinanet import retinanet_bbox\n",
    "from keras_retinanet.bin.train import create_models\n",
    "\n",
    "## Load the param file\n",
    "anchor_params = parse_anchor_parameters(config)\n",
    "\n",
    "## Load the weights file\n",
    "model_path = \"../../weight_files_imp/resnet50_coco_best_v2.1.0.h5\"\n",
    "#weights = backbone.download_imagenet()\n",
    "\n",
    "## Create models with pretrained weights and other details.\n",
    "## we are using only 1 gpu, and set the learning rate as \n",
    "model, training_model, prediction_model = create_models(\n",
    "    backbone_retinanet=backbone.retinanet,\n",
    "    num_classes=train_generator.num_classes(),\n",
    "    weights=model_path,\n",
    "    multi_gpu=0,\n",
    "    freeze_backbone=False,\n",
    "    lr=1e-5,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.callbacks.eval import Evaluate\n",
    "from keras_retinanet.callbacks import RedirectModel\n",
    "\n",
    "## Initialize the evaluator\n",
    "evaluation = Evaluate(validation_generator, weighted_average=True)\n",
    "evaluation = RedirectModel(evaluation, prediction_model)\n",
    "\n",
    "## Model weights (Create this folder if not present)\n",
    "snapshot_path = \"model_weights/\"\n",
    "\n",
    "if not os.path.exists(snapshot_path):\n",
    "    os.makedirs(snapshot_path)\n",
    "\n",
    "# Our dataset type is csv\n",
    "dataset_type = \"csv\"\n",
    "\n",
    "# Define checkpoint\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(\n",
    "                snapshot_path,\n",
    "                '{backbone}_{dataset_type}_{{epoch:02d}}.h5'.format(backbone=backbone, dataset_type=\"csv\")\n",
    "            ),\n",
    "            verbose=1,\n",
    "            # save_best_only=True,\n",
    "            # monitor=\"mAP\",\n",
    "            # mode='max'\n",
    "        )\n",
    "\n",
    "# Create checkpoint to store model weights after each epoch\n",
    "checkpoint = RedirectModel(checkpoint, model)\n",
    "\n",
    "## create a schedular. We will be using ReduceLRONplateau\n",
    "sched = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor    = 'loss',\n",
    "        factor     = 0.1,\n",
    "        patience   = 2,\n",
    "        verbose    = 1,\n",
    "        mode       = 'auto',\n",
    "        min_delta  = 0.0001,\n",
    "        cooldown   = 0,\n",
    "        min_lr     = 0\n",
    ")\n",
    "\n",
    "## Set all the call backs\n",
    "callbacks = [evaluation, checkpoint, sched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2200/2200 [==============================] - 1296s 589ms/step - loss: 0.9280 - regression_loss: 0.7008 - classification_loss: 0.2272 - val_loss: 0.8157 - val_regression_loss: 0.6702 - val_classification_loss: 0.1455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:02:43 Time:  0:02:43\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8719\n",
      "mAP: 0.8719\n",
      "\n",
      "Epoch 00001: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_01.h5\n",
      "Epoch 2/10\n",
      "2200/2200 [==============================] - 969s 441ms/step - loss: 0.7742 - regression_loss: 0.6473 - classification_loss: 0.1269 - val_loss: 0.8019 - val_regression_loss: 0.6776 - val_classification_loss: 0.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:26 Time:  0:01:26\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8721\n",
      "mAP: 0.8721\n",
      "\n",
      "Epoch 00002: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_02.h5\n",
      "Epoch 3/10\n",
      "2200/2200 [==============================] - 970s 441ms/step - loss: 0.7371 - regression_loss: 0.6222 - classification_loss: 0.1149 - val_loss: 0.8869 - val_regression_loss: 0.6976 - val_classification_loss: 0.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:26 Time:  0:01:26\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8741\n",
      "mAP: 0.8741\n",
      "\n",
      "Epoch 00003: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_03.h5\n",
      "Epoch 4/10\n",
      " 986/2200 [============>.................] - ETA: 8:27 - loss: 0.7125 - regression_loss: 0.6096 - classification_loss: 0.1028764 instances of class football with average precision: 0.8767\n",
      "mAP: 0.8767\n",
      "\n",
      "Epoch 00004: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_04.h5\n",
      "Epoch 5/10\n",
      "2200/2200 [==============================] - 971s 441ms/step - loss: 0.6695 - regression_loss: 0.5810 - classification_loss: 0.0885 - val_loss: 0.8145 - val_regression_loss: 0.6487 - val_classification_loss: 0.1658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:26 Time:  0:01:26\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8791\n",
      "mAP: 0.8791\n",
      "\n",
      "Epoch 00005: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_05.h5\n",
      "Epoch 6/10\n",
      "2200/2200 [==============================] - 975s 443ms/step - loss: 0.6672 - regression_loss: 0.5807 - classification_loss: 0.0865 - val_loss: 0.9485 - val_regression_loss: 0.6783 - val_classification_loss: 0.2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:26 Time:  0:01:26\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8767\n",
      "mAP: 0.8767\n",
      "\n",
      "Epoch 00006: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_06.h5\n",
      "Epoch 7/10\n",
      "2200/2200 [==============================] - 971s 441ms/step - loss: 0.6485 - regression_loss: 0.5681 - classification_loss: 0.0805 - val_loss: 0.8563 - val_regression_loss: 0.6825 - val_classification_loss: 0.1739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:25 Time:  0:01:25\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8750\n",
      "mAP: 0.8750\n",
      "\n",
      "Epoch 00007: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_07.h5\n",
      "Epoch 8/10\n",
      "2200/2200 [==============================] - 974s 443ms/step - loss: 0.6395 - regression_loss: 0.5636 - classification_loss: 0.0759 - val_loss: 0.8897 - val_regression_loss: 0.6522 - val_classification_loss: 0.2375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:26 Time:  0:01:26\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8835\n",
      "mAP: 0.8835\n",
      "\n",
      "Epoch 00008: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_08.h5\n",
      "Epoch 9/10\n",
      "2200/2200 [==============================] - 971s 442ms/step - loss: 0.6235 - regression_loss: 0.5489 - classification_loss: 0.0746 - val_loss: 0.9073 - val_regression_loss: 0.6703 - val_classification_loss: 0.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:26 Time:  0:01:26\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8786\n",
      "mAP: 0.8786\n",
      "\n",
      "Epoch 00009: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_09.h5\n",
      "Epoch 10/10\n",
      "2200/2200 [==============================] - 970s 441ms/step - loss: 0.6199 - regression_loss: 0.5480 - classification_loss: 0.0719 - val_loss: 0.8621 - val_regression_loss: 0.6570 - val_classification_loss: 0.2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (629 of 629) |#####| Elapsed Time: 0:01:26 Time:  0:01:26\n",
      "Parsing annotations: 100% (629 of 629) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 instances of class football with average precision: 0.8772\n",
      "mAP: 0.8772\n",
      "\n",
      "Epoch 00010: saving model to model_weights/<keras_retinanet.models.resnet.ResNetBackbone object at 0x7fa37efcb470>_csv_10.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa37925b6d8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train for 10 epochs\n",
    "training_model.fit_generator(\n",
    "        generator=train_generator,\n",
    "        steps_per_epoch=2200,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        workers=8,\n",
    "        use_multiprocessing=True,\n",
    "        max_queue_size=10,\n",
    "        validation_data=validation_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
