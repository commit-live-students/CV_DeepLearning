Q1. Naive Bayes classifier is an example of?

A) Generative Model. 
B) Discrminative Model 
C) Both A and B 
D) None of the above

Ans. A) Naive bayes classifier is based on bayes theorum which is the equation for a generative model.

Q2. Answer True or False

Image to Image Translation is an example of discriminative model.

A) True 
B) False

Ans. False. Image to image translation required new data generation.

Q3. Answer True of False 

A generative model tries to predict conditional probability  P(Y|X) along with probability of  P(Y)   

A) True 
B) False

Ans. A) True. A generative model always predicts both the probabilities.

Q4. How will you train generator network in a GAN architecture?

A) With Discriminator.
B) Without Discriminator.
C) By Training whole GAN at once.
D) None of the Above

Ans. B) Generator will be trained using the error produced by discriminator. During training of generator, discriminator's weights will not be trained.

Q5. What does batchnormalization (BN) layer do in a network?

A) Normalize input batches.
B) Reduces covariate shift from the input.
C) Both A and B.
D) None of the above.

Ans. C) BN reduces covariate shift by normalizing the input data.

Q6. What kind of cost function is used to train a GAN?

A) Categorical Cross-entropy loss.
B) Mean Square Error.
C) Binary Cross-entropy loss.
D) None of the above

Ans. C) Binary cross entropy loss is used to train a GAN.

Q7. Answer True or False.

Word 'Latent Variables' is used for hidden layer variables.

A) True.
B) False.

Ans. A) True. Hidden layer variable also known as latent variables in neural networks.

Q8. What kind of Data Compression is generated by Auto-encoders?

A) Lossy Compression similar to JPEG.
B) Loss-less compression similar to PNG.
C) Depends on number of latent variables.
D) None of the above.

Ans. A) Auto-encoders always provide lossy data compression due raduced number of parameters in latent space. If we will increase number of latent variables, compression exercise would become meaning less.

Q9. Answer True or False.

A variational autoencoder generates a discrete latent space.

A) True.
B) False.

Ans. B) False. VAE is used for generating continuous latent space.

Q10. Answer True or False.

We can train a neural network by optimizing more than one loss function in single epoch.

A) True
B) False

Ans. A) True. Yes we can optimize two different loss functions at the same time. We have did it during training of VAE. where we have tuned binary cross entropy loss and KL divergence loss at the same time.