{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "## Motivation\n",
    "In this chapter we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification.\n",
    "\n",
    "For example, in the image below an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as “cat”.\n",
    "\n",
    "<img src=http://cs231n.github.io/assets/classify.png>\n",
    "\n",
    "> The task in Image Classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue.\n",
    "\n",
    "## Challenges. \n",
    "Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. Recall that the raw representation of images as a 3-D array of brightness values:\n",
    "\n",
    "- __Viewpoint variation__ A single instance of an object can be oriented in many ways with respect to the camera.\n",
    "- __Scale variation__ Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).\n",
    "- __Deformation__ Many objects of interest are not rigid bodies and can be deformed in extreme ways.\n",
    "- __Occlusion__ The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.\n",
    "- __Illumination conditions__ The effects of illumination are drastic on the pixel level.\n",
    "- __Background clutter__ The objects of interest may blend into their environment, making them hard to identify.\n",
    "- __Intra-class variation__ The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance.\n",
    "\n",
    "A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.\n",
    "\n",
    "<img src=http://cs231n.github.io/assets/challenges.jpeg>\n",
    "\n",
    "## Data-driven approach\n",
    "How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one you would take with a child: we’re going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a data-driven approach, since it relies on first accumulating a training dataset of labeled images. Here is an example of what such a dataset might look like:\n",
    "\n",
    "<img src=http://cs231n.github.io/assets/trainset.jpg>\n",
    "\n",
    "> An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category.\n",
    "\n",
    "## The image classification pipeline\n",
    "We’ve seen that the task in Image Classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows:\n",
    "\n",
    "- __Input__ Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set.\n",
    "- __Learning__ Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model.\n",
    "- __Evaluation__ In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color and geometric transforms\n",
    "\n",
    "## Changing Colourspaces\n",
    "\n",
    "### Goal\n",
    "In this tutorial, you will learn how to convert images from one color-space to another, like `BGR` $\\leftrightarrow$ `Gray`, `BGR` $\\leftrightarrow$ `HSV` etc.  \n",
    "In addition to that, we will create an application which extracts a colored object in a video  \n",
    "You will learn following functions : `cv2.cvtColor()`, `cv2.inRange()` etc.  \n",
    "\n",
    "### Changing Color-space\n",
    "There are more than 150 color-space conversion methods available in OpenCV. But we will look into only two which are most widely used ones, `BGR` $\\leftrightarrow$ `Gray` and `BGR` $\\leftrightarrow$ `HSV`.\n",
    "\n",
    "For color conversion, we use the function `cv2.cvtColor(input_image, flag)` where flag determines the type of conversion.\n",
    "\n",
    "For `BGR` $\\rightarrow$ `Gray` conversion we use the flags `cv2.COLOR_BGR2GRAY`. Similarly for `BGR` $\\rightarrow$ `HSV`, we use the flag `cv2.COLOR_BGR2HSV`. To get other flags, just run following commands in your Python terminal :\n",
    "\n",
    "```\n",
    ">>> import cv2\n",
    ">>> flags = [i for i in dir(cv2) if i.startswith('COLOR_')]\n",
    ">>> print flags\n",
    "```\n",
    "\n",
    "__Note__:  \n",
    "For HSV, Hue range is [0,179], Saturation range is [0,255] and Value range is [0,255]. Different softwares use different scales. So if you are comparing OpenCV values with them, you need to normalize these ranges.\n",
    "\n",
    "\n",
    "### Application: Object Tracking\n",
    "Now we know how to convert BGR image to HSV, we can use this to extract a colored object. In HSV, it is more easier to represent a color than RGB color-space. In our application, we will try to extract a blue colored object. So here is the method:\n",
    "- Take each frame of the video\n",
    "- Convert from BGR to HSV color-space\n",
    "- We threshold the HSV image for a range of blue color\n",
    "- Now extract the blue object alone, we can do whatever on that image we want.\n",
    "\n",
    "Below is the code which are commented in detail :\n",
    "\n",
    "```\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    # Take each frame\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Convert BGR to HSV\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # define range of blue color in HSV\n",
    "    lower_blue = np.array([110,50,50])\n",
    "    upper_blue = np.array([130,255,255])\n",
    "\n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "__How to find HSV values to track?__  \n",
    "It is very simple and you can use the same function, cv2.cvtColor(). Instead of passing an image, you just pass the BGR values you want. For example, to find the HSV value of Green, try following commands in Python terminal:\n",
    "\n",
    "```\n",
    ">>> green = np.uint8([[[0,255,0 ]]])\n",
    ">>> hsv_green = cv2.cvtColor(green,cv2.COLOR_BGR2HSV)\n",
    ">>> print hsv_green\n",
    "[[[ 60 255 255]]]\n",
    "```\n",
    "\n",
    "Now you take [H-10, 100,100] and [H+10, 255, 255] as lower bound and upper bound respectively. Apart from this method, you can use any image editing tools like GIMP or any online converters to find these values, but don’t forget to adjust the HSV ranges.\n",
    "\n",
    "## Geometric Transformation of Images\n",
    "\n",
    "### Goals\n",
    "Learn to apply different geometric transformation to images like translation, rotation, affine transformation etc.  \n",
    "You will see these functions: `cv2.getPerspectiveTransform`\n",
    "\n",
    "### Transformations\n",
    "OpenCV provides two transformation functions, `cv2.warpAffine` and `cv2.warpPerspective`, with which you can have all kinds of transformations. `cv2.warpAffine` takes a `2x3` transformation matrix while `cv2.warpPerspective` takes a `3x3` transformation matrix as input.\n",
    "\n",
    "__Scaling__ \n",
    "Scaling is just resizing of the image. OpenCV comes with a function `cv2.resize()` for this purpose. The size of the image can be specified manually, or you can specify the scaling factor. Different interpolation methods are used. Preferable interpolation methods are `cv2.INTER_AREA` for shrinking and `cv2.INTER_CUBIC` (slow) & `cv2.INTER_LINEAR` for zooming. By default, interpolation method used is `cv2.INTER_LINEAR` for all resizing purposes. You can resize an input image either of following methods:\n",
    "\n",
    "```\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('messi5.jpg')\n",
    "\n",
    "res = cv2.resize(img,None,fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "#OR\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "res = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n",
    "```\n",
    "\n",
    "__Translation__  \n",
    "Translation is the shifting of object’s location. If you know the shift in `(x,y)` direction, let it be `(t_x,t_y)`, you can create the transformation matrix $\\textbf{M}$ as follows:\n",
    "\n",
    "$$ M = \\begin{bmatrix} 1 & 0 & t_x \\\\ 0 & 1 & t_y  \\end{bmatrix} $$\n",
    "\n",
    "You can take make it into a Numpy array of type `np.float32` and pass it into `cv2.warpAffine()` function. See below example for a shift of (100,50):\n",
    "\n",
    "```\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('messi5.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = np.float32([[1,0,100],[0,1,50]])\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "\n",
    "cv2.imshow('img',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "_Warning_: Third argument of the `cv2.warpAffine()` function is the size of the output image, which should be in the form of (width, height). Remember width = number of columns, and height = number of rows.\n",
    "\n",
    "See the result below:\n",
    "<img src=https://opencv-python-tutroals.readthedocs.io/en/latest/_images/translation.jpg>\n",
    "\n",
    "__Rotation__  \n",
    "Rotation of an image for an angle $\\theta$ is achieved by the transformation matrix of the form\n",
    "\n",
    "$$ M = \\begin{bmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta & cos\\theta   \\end{bmatrix} $$\n",
    "\n",
    "But OpenCV provides scaled rotation with adjustable center of rotation so that you can rotate at any location you prefer. Modified transformation matrix is given by\n",
    "\n",
    "$$ \\begin{bmatrix} \\alpha &  \\beta & (1- \\alpha )  \\cdot center.x -  \\beta \\cdot center.y \\\\ - \\beta &  \\alpha &  \\beta \\cdot center.x + (1- \\alpha )  \\cdot center.y \\end{bmatrix} $$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\begin{array}{l} \\alpha =  scale \\cdot \\cos \\theta , \\\\ \\beta =  scale \\cdot \\sin \\theta \\end{array} $$\n",
    "\n",
    "To find this transformation matrix, OpenCV provides a function, `cv2.getRotationMatrix2D`. Check below example which rotates the image by 90 degree with respect to center without any scaling.\n",
    "\n",
    "```\n",
    "img = cv2.imread('messi5.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "```\n",
    "\n",
    "See the result:\n",
    "\n",
    "<img src=https://opencv-python-tutroals.readthedocs.io/en/latest/_images/rotation.jpg>\n",
    "\n",
    "__Affine Transformation__ \n",
    "In affine transformation, all parallel lines in the original image will still be parallel in the output image. To find the transformation matrix, we need three points from input image and their corresponding locations in output image. Then `cv2.getAffineTransform` will create a `2x3` matrix which is to be passed to `cv2.warpAffine`.\n",
    "\n",
    "Check below example, and also look at the selected points (which are marked in Green color):\n",
    "\n",
    "```\n",
    "img = cv2.imread('drawing.png')\n",
    "rows,cols,ch = img.shape\n",
    "\n",
    "pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "pts2 = np.float32([[10,100],[200,50],[100,250]])\n",
    "\n",
    "M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "See the result:\n",
    "<img src=https://opencv-python-tutroals.readthedocs.io/en/latest/_images/affine.jpg>\n",
    "\n",
    "__Perspective Transformation__  \n",
    "For perspective transformation, you need a `3x3` transformation matrix. Straight lines will remain straight even after the transformation. To find this transformation matrix, you need 4 points on the input image and corresponding points on the output image. Among these 4 points, 3 of them should not be collinear. Then transformation matrix can be found by the function `cv2.getPerspectiveTransform`. Then apply `cv2.warpPerspective` with this 3x3 transformation matrix.\n",
    "\n",
    "See the code below:\n",
    "```\n",
    "img = cv2.imread('sudokusmall.png')\n",
    "rows,cols,ch = img.shape\n",
    "\n",
    "pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])\n",
    "pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "dst = cv2.warpPerspective(img,M,(300,300))\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Result:\n",
    "<img src=https://opencv-python-tutroals.readthedocs.io/en/latest/_images/perspective.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Using KNN\n",
    "\n",
    "### Nearest Neighbor Classifier\n",
    "**The CIFAR-10 Dataset** One popular image classification dataset is the <a href=\"http://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 dataset</a>. This dataset consists of 60,000 tiny images that are 32 pixels high and wide. Each image is labeled with one of 10 classes (for example *\"airplane, automobile, bird, etc\"*). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images. In the image below you can see 10 random example images from each one of the 10 classes:\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn.jpg\"/>\n",
    "\n",
    "> Left: Example images from the <a href=\"http://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 dataset</a>. Right: first column shows a few test images and next to each are shown the top 10 nearest neighbors in the training set according to pixel-wise difference.\n",
    "\n",
    "Suppose now that we are given the CIFAR-10 training set of 50,000 images (5,000 images for every one of the labels), and we wish to label the remaining 10,000. The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image. In the image above and on the right you can see an example result of such a procedure for 10 example test images. Notice that in only about 3 out of 10 examples an image of the same class is retrieved, while in the other 7 examples this is not the case. For example, in the 8th row the nearest training image to the horse head is a red car, presumably due to the strong black background. As a result, this image of a horse would in this case be mislabeled as a car.\n",
    "\n",
    "You may have noticed that we left unspecified the details of exactly how we compare two images, which in this case are just two blocks of 32 x 32 x 3. One of the simplest possibilities is to compare the images pixel by pixel and add up all the differences. In other words, given two images and representing them as vectors \\\\( I_1, I_2 \\\\) , a reasonable choice for comparing them might be the **L1 distance**:\n",
    "\n",
    "$$\n",
    "d_1 (I_1, I_2) = \\sum_{p} \\left| I^p_1 - I^p_2 \\right|\n",
    "$$\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def train(self, X, y):\n",
    "    \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n",
    "    # the nearest neighbor classifier simply remembers all the training data\n",
    "    self.Xtr = X\n",
    "    self.ytr = y\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    # lets make sure that the output type matches the input type\n",
    "    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n",
    "\n",
    "    # loop over all test rows\n",
    "    for i in xrange(num_test):\n",
    "      # find the nearest training image to the i'th test image\n",
    "      # using the L1 distance (sum of absolute value differences)\n",
    "      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n",
    "      min_index = np.argmin(distances) # get the index with smallest distance\n",
    "      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "    return Ypred\n",
    "```\n",
    "\n",
    "**The choice of distance.** \n",
    "There are many other ways of computing distances between vectors. Another common choice could be to instead use the **L2 distance**, which has the geometric interpretation of computing the euclidean distance between two vectors. The distance takes the form:\n",
    "\n",
    "$$\n",
    "d_2 (I_1, I_2) = \\sqrt{\\sum_{p} \\left( I^p_1 - I^p_2 \\right)^2}\n",
    "$$\n",
    "\n",
    "In other words we would be computing the pixelwise difference as before, but this time we square all of them, add them up and finally take the square root. In numpy, using the code from above we would need to only replace a single line of code. The line that computes the distances:\n",
    "\n",
    "```python\n",
    "distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))\n",
    "```\n",
    "\n",
    "### k - Nearest Neighbor Classifier\n",
    "\n",
    "You may have noticed that it is strange to only use the label of the nearest image when we wish to make a prediction. Indeed, it is almost always the case that one can do better by using what's called a __k-Nearest Neighbor Classifier__. The idea is very simple: instead of finding the single closest image in the training set, we will find the top **k** closest images, and have them vote on the label of the test image. In particular, when _k = 1_, we recover the Nearest Neighbor classifier. Intuitively, higher values of __k__ have a smoothing effect that makes the classifier more resistant to outliers:\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/knn.jpeg\">\n",
    "    \n",
    "> An example of the difference between Nearest Neighbor and a 5-Nearest Neighbor classifier, using 2-dimensional points and 3 classes (red, blue, green). The colored regions show the <b>decision boundaries</b> induced by the classifier with an L2 distance. The white regions show points that are ambiguously classified (i.e. class votes are tied for at least two classes). Notice that in the case of a NN classifier, outlier datapoints (e.g. green point in the middle of a cloud of blue points) create small islands of likely incorrect predictions, while the 5-NN classifier smooths over these irregularities, likely leading to better <b>generalization</b> on the test data (not shown). Also note that the gray regions in the 5-NN image are caused by ties in the votes among the nearest neighbors (e.g. 2 neighbors are red, next two neighbors are blue, last neighbor is green).\n",
    "\n",
    "In practice, you will almost always want to use k-Nearest Neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "# !tar xvfz cifar-10-python.tar.gz\n",
    "# !ls\n",
    "\n",
    "# !pip3 install opencv-python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "d1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "d2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "d3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "d4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "d5 = unpickle('cifar-10-batches-py/data_batch_5')\n",
    "t = unpickle('cifar-10-batches-py/test_batch')\n",
    "\n",
    "trainData = np.concatenate([d1[b'data'], d2[b'data'], d3[b'data'], d4[b'data'], d5[b'data']])\n",
    "trainLabels = np.concatenate([d1[b'labels'], d2[b'labels'], d3[b'labels'], d4[b'labels'], d5[b'labels']])\n",
    "\n",
    "trainData = trainData.astype(np.float32)\n",
    "trainLabels = trainLabels.astype(np.float32)\n",
    "\n",
    "knn = cv2.ml.KNearest_create()\n",
    "knn.train(trainData, cv2.ml.ROW_SAMPLE, trainLabels)\n",
    "\n",
    "ret, results, neighbours ,dist = knn.findNearest(t[b'data'].astype(np.float32), k=3)\n",
    "\n",
    "y = np.reshape(np.array(t[b'labels']), [-1,1])\n",
    "np.sum(results==y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification\n",
    "\n",
    "Recall, Image Classification is the task of assigning a single label to an image from a fixed set of categories. Also remember that k-Nearest Neighbor (kNN) classifier labels images by comparing them to (annotated) images from the training set. As we saw, kNN has a number of disadvantages:\n",
    "\n",
    "- The classifier must *remember* all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.\n",
    "- Classifying a test image is expensive since it requires a comparison to all training images.\n",
    "\n",
    "**Overview**. We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a **score function** that maps the raw data to class scores, and a **loss function** that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.\n",
    "\n",
    "### Parameterized mapping from images to label scores\n",
    "\n",
    "The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. Let's assume a training dataset of images \\\\( x_i \\in R^D \\\\), each associated with a label \\\\( y_i \\\\). Here \\\\( i = 1 \\dots N \\\\) and \\\\( y_i \\in \\{ 1 \\dots K \\} \\\\). That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function \\\\(f: R^D \\mapsto R^K\\\\) that maps the raw image pixels to class scores.\n",
    "\n",
    "**Linear classifier.** We will start out with arguably the simplest possible function, a linear mapping:\n",
    "\n",
    "$$\n",
    "f(x_i, W, b) =  W x_i + b\n",
    "$$\n",
    "\n",
    "In the above equation, we are assuming that the image \\\\(x_i\\\\) has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the **parameters** of the function. In CIFAR-10, \\\\(x_i\\\\) contains all pixels in the i-th image flattened into a single [3072 x 1] column, W is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in W are often called the **weights**, and b is called the **bias vector** because it influences the output scores, but without interacting with the actual data \\\\(x_i\\\\).\n",
    "\n",
    "There are a few things to note:\n",
    "\n",
    "- First, note that the single matrix multiplication \\\\(W x_i\\\\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of **W**.\n",
    "- Notice also that we think of the input data \\\\( (x_i, y_i) \\\\) as given and fixed, but we have control over the setting of the parameters **W,b**. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes.\n",
    "- An advantage of this approach is that the training data is used to learn the parameters **W,b**, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.\n",
    "- Lastly, note that classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a linear classifier\n",
    "\n",
    "Notice that a linear classifier computes the score of a class as a weighted sum of all of its pixel values across all 3 of its color channels. Depending on precisely what values we set for these weights, the function has the capacity to like or dislike (depending on the sign of each weight) certain colors at certain positions in the image. For instance, you can imagine that the \"ship\" class might be more likely if there is a lot of blue on the sides of an image (which could likely correspond to water). You might expect that the \"ship\" classifier would then have a lot of positive weights across its blue channel weights (presence of blue increases score of ship), and negative weights in the red/green channels (presence of red/green decreases the score of ship).\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/imagemap.jpg\">\n",
    "\n",
    "> An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it's looking at a dog.\n",
    "\n",
    "**Analogy of images as high-dimensional points.** Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). Analogously, the entire dataset is a (labeled) set of points.\n",
    "\n",
    "Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. We cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing:\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/pixelspace.jpeg\">\n",
    "\n",
    "> Cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. Using the example of the car classifier (in red), the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores.\n",
    "\n",
    "As we saw above, every row of \\\\(W\\\\) is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of \\\\(W\\\\), the corresponding line in the pixel space will rotate in different directions. The biases \\\\(b\\\\), on the other hand, allow our classifiers to translate the lines. In particular, note that without the bias terms, plugging in \\\\( x_i = 0 \\\\) would always give score of zero regardless of the weights, so all lines would be forced to cross the origin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Matching in OpenCV\n",
    "\n",
    "Template Matching is a method for searching and finding the location of a template image in a larger image. OpenCV comes with a function `cv.matchTemplate()` for this purpose. It simply slides the template image over the input image and compares the template and patch of input image under the template image. Several comparison methods are implemented in OpenCV. It returns a grayscale image, where each pixel denotes how much does the neighbourhood of that pixel match with template.\n",
    "\n",
    "If input image is of size (WxH) and template image is of size (wxh), output image will have a size of (W-w+1, H-h+1). Once you got the result, you can use cv.minMaxLoc() function to find where is the maximum/minimum value. Take it as the top-left corner of rectangle and take (w,h) as width and height of the rectangle. That rectangle is your region of template.\n",
    "\n",
    "#### Template Matching in OpenCV\n",
    "Here, as an example, we will search for Messi's face in his photo. So we created a template as below:\n",
    "\n",
    "<img src=https://docs.opencv.org/3.4/messi_face.jpg>\n",
    "\n",
    "We will try all the comparison methods so that we can see how their results look like:\n",
    "\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('messi.jpg',0)\n",
    "img2 = img.copy()\n",
    "template = cv.imread('template.jpg',0)\n",
    "w, h = template.shape[::-1]\n",
    "# All the 6 methods for comparison in a list\n",
    "methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',\n",
    "            'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']\n",
    "for meth in methods:\n",
    "    img = img2.copy()\n",
    "    method = eval(meth)\n",
    "    # Apply template Matching\n",
    "    res = cv.matchTemplate(img,template,method)\n",
    "    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)\n",
    "    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "    cv.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "    plt.subplot(121),plt.imshow(res,cmap = 'gray')\n",
    "    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122),plt.imshow(img,cmap = 'gray')\n",
    "    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "    plt.suptitle(meth)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "See the results below:\n",
    "cv.TM_CCOEFF\n",
    "<img src=https://docs.opencv.org/3.4/template_ccoeff_1.jpg>\n",
    "cv.TM_CCOEFF_NORMED\n",
    "<img src=https://docs.opencv.org/3.4/template_ccoeffn_2.jpg>\n",
    "cv.TM_CCORR\n",
    "<img src=https://docs.opencv.org/3.4/template_ccorr_3.jpg>\n",
    "cv.TM_CCORR_NORMED\n",
    "<img src=https://docs.opencv.org/3.4/template_ccorrn_4.jpg>\n",
    "cv.TM_SQDIFF\n",
    "<img src=https://docs.opencv.org/3.4/template_sqdiff_5.jpg>\n",
    "cv.TM_SQDIFF_NORMED\n",
    "<img src=https://docs.opencv.org/3.4/template_sqdiffn_6.jpg>\n",
    "\n",
    "You can see that the result using cv.TM_CCORR is not good as we expected.\n",
    "\n",
    "#### Template Matching with Multiple Objects\n",
    "We searched image for Messi's face, which occurs only once in the image. Suppose you are searching for an object which has multiple occurrences, `cv.minMaxLoc()` won't give you all the locations. In that case, we will use thresholding. So in this example, we will use a screenshot of the famous game Mario and we will find the coins in it.\n",
    "\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img_rgb = cv.imread('mario.png')\n",
    "img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)\n",
    "template = cv.imread('mario_coin.png',0)\n",
    "w, h = template.shape[::-1]\n",
    "res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)\n",
    "threshold = 0.8\n",
    "loc = np.where( res >= threshold)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n",
    "cv.imwrite('res.png',img_rgb)\n",
    "```\n",
    "Result:\n",
    "<img src=https://docs.opencv.org/3.4/res_mario.jpg>\n",
    "\n",
    "**Interpretation of linear classifiers as template matching.** Another interpretation for the weights W is that each row of W corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that “fits” best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we use the (negative) inner product as the distance instead of the L1 or L2 distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Recall that we had defined a function from the pixel values to class scores, which was parameterized by a set of weights \\\\(W\\\\). Moreover, we saw that we don't have control over the data \\\\( (x_i,y_i) \\\\) (it is fixed and given), but we do have control over these weights and we want to set them so that the predicted class scores are consistent with the ground truth labels in the training data.\n",
    "\n",
    "For example, going back to the example image of a cat and its scores for the classes \"cat\", \"dog\" and \"ship\", we saw that the particular set of weights in that example was not very good at all: We fed in the pixels that depict a cat but the cat score came out very low (-96.8) compared to the other classes (dog score 437.9 and ship score 61.95). We are going to measure our unhappiness with outcomes such as this one with a **loss function** (or sometimes also referred to as the **cost function** or the **objective**). Intuitively, the loss will be high if we're doing a poor job of classifying the training data, and it will be low if we're doing well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Support Vector Machine loss\n",
    "\n",
    "There are several ways to define the details of the loss function. As a first example we will first develop a commonly used loss called the **Multiclass Support Vector Machine** (SVM) loss. The SVM loss is set up so that the SVM \"wants\" the correct class for each image to a have a score higher than the incorrect classes by some fixed margin \\\\(\\Delta\\\\). Notice that it's sometimes helpful to describe the loss functions as we did above: The SVM \"wants\" a certain outcome in the sense that the outcome would yield a lower loss (which is good).\n",
    "\n",
    "Let's now get more precise. Recall that for the i-th example we are given the pixels of image \\\\( x_i \\\\) and the label \\\\( y_i \\\\) that specifies the index of the correct class. The score function takes the pixels and computes the vector \\\\( f(x_i, W) \\\\) of class scores, which we will abbreviate to \\\\(s\\\\).  For example, the score for the j-th class is the j-th element: \\\\( s_j = f(x_i, W)_j \\\\). The Multiclass SVM loss for the i-th example is then formalized as follows:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\n",
    "$$\n",
    "\n",
    "**Example.** Lets unpack this with an example to see how it works. Suppose that we have three classes that receive the scores \\\\( s = [13, -7, 11]\\\\), and that the first class is the true class (i.e. \\\\(y_i = 0\\\\)). Also assume that \\\\(\\Delta\\\\) is 10. The expression above sums over all incorrect classes (\\\\(j \\neq y_i\\\\)), so we get two terms:\n",
    "\n",
    "$$\n",
    "L_i = \\max(0, -7 - 13 + 10) + \\max(0, 11 - 13 + 10)\n",
    "$$\n",
    "\n",
    "You can see that the first term gives zero since [-7 - 13 + 10] gives a negative number, which is then thresholded to zero with the \\\\(max(0,-)\\\\) function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; Any additional difference above the margin is clamped at zero with the max operation. The second term computes [11 - 13 + 10] which gives 8. That is, even though the correct class had a higher score than the incorrect class (13 > 11), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class \\\\(y_i\\\\) to be larger than the incorrect class scores by at least by \\\\(\\Delta\\\\) (delta). If this is not the case, we will accumulate loss.\n",
    "\n",
    "Note that in this particular module we are working with linear score functions ( \\\\( f(x_i; W) =  W x_i \\\\) ), so we can also rewrite the loss function in this equivalent form:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} \\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta)\n",
    "$$\n",
    "\n",
    "where \\\\(w_j\\\\) is the j-th row of \\\\(W\\\\) reshaped as a column. However, this will not necessarily be the case once we start to consider more complex forms of the score function \\\\(f\\\\).\n",
    "\n",
    "A last piece of terminology we'll mention before we finish with this section is that the threshold at zero \\\\(max(0,-)\\\\) function is often called the **hinge loss**. You'll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form \\\\(max(0,-)^2\\\\) that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.\n",
    "\n",
    "> The loss function quantifies our unhappiness with predictions on the training set\n",
    "\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/margin.jpg\">\n",
    "\n",
    "> The Multiclass Support Vector Machine \"wants\" the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible.\n",
    "\n",
    "**Regularization**. There is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters **W** that correctly classify every example (i.e. all scores are so that all the margins are met, and \\\\(L_i = 0\\\\) for all i). The issue is that this set of **W** is not necessarily unique: there might be many similar **W** that correctly classify the examples. One easy way to see this is that if some parameters **W** correctly classify all examples (so loss is zero for each example), then any multiple of these parameters \\\\( \\lambda W \\\\) where \\\\( \\lambda > 1 \\\\) will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of **W** by 2 would make the new difference 30.\n",
    "\n",
    "In other words, we wish to encode some preference for a certain set of weights **W** over others to remove this ambiguity. We can do so by extending the loss function with a **regularization penalty** \\\\(R(W)\\\\). The most common regularization penalty is the **L2** norm that discourages large weights through an elementwise quadratic penalty over all parameters:\n",
    "\n",
    "$$\n",
    "R(W) = \\sum_k\\sum_l W_{k,l}^2\n",
    "$$\n",
    "\n",
    "In the expression above, we are summing up all the squared elements of \\\\(W\\\\). Notice that the regularization function is not a function of the data, it is only based on the weights. Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the **data loss** (which is the average loss \\\\(L_i\\\\) over all examples) and the **regularization loss**. That is, the full Multiclass SVM loss becomes:\n",
    "\n",
    "$$\n",
    "L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\\n",
    "$$\n",
    "\n",
    "Or expanding this out in its full form:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2\n",
    "$$\n",
    "\n",
    "Where \\\\(N\\\\) is the number of training examples. As you can see, we append the regularization penalty to the loss objective, weighted by a hyperparameter \\\\(\\lambda\\\\). There is no simple way of setting this hyperparameter and it is usually determined by cross-validation.\n",
    "\n",
    "The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector \\\\(x = [1,1,1,1] \\\\) and two weight vectors \\\\(w_1 = [1,0,0,0]\\\\), \\\\(w_2 = [0.25,0.25,0.25,0.25] \\\\). Then \\\\(w_1^Tx = w_2^Tx = 1\\\\) so both weight vectors lead to the same dot product, but the L2 penalty of \\\\(w_1\\\\) is 1.0 while the L2 penalty of \\\\(w_2\\\\) is only 0.25. Therefore, according to the L2 penalty the weight vector \\\\(w_2\\\\) would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in \\\\(w_2\\\\) are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less *overfitting*.\n",
    "\n",
    "Note that biases do not have the same effect since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights \\\\(W\\\\) but not the biases \\\\(b\\\\). However, in practice this often turns out to have a negligible effect. Lastly, note that due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, because this would only be possible in the pathological setting of \\\\(W = 0\\\\).\n",
    "\n",
    "**Code**. Here is the loss function (without regularization) implemented in Python, in both unvectorized and half-vectorized form:\n",
    "\n",
    "```python\n",
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in xrange(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i\n",
    "\n",
    "def L(X, y, W):\n",
    "  \"\"\"\n",
    "  fully-vectorized implementation :\n",
    "  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "  - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "  - W are weights (e.g. 10 x 3073)\n",
    "  \"\"\"\n",
    "  # evaluate loss over all examples in X without using any for loops\n",
    "  # left as exercise to reader in the assignment\n",
    "```\n",
    "\n",
    "The takeaway from this section is that the SVM loss takes one particular approach to measuring how consistent the predictions on training data are with the ground truth labels. Additionally, making good predictions on the training set is equivalent to minimizing the loss.\n",
    "\n",
    "> All we have to do now is to come up with a way to find the weights that minimize the loss.\n",
    "\n",
    "**Setting Delta.** Note that we brushed over the hyperparameter \\\\(\\Delta\\\\) and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to \\\\(\\Delta = 1.0\\\\) in all cases. The hyperparameters \\\\(\\Delta\\\\) and \\\\(\\lambda\\\\) seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights \\\\(W\\\\) has direct effect on the scores (and hence also their differences): As we shrink all values inside \\\\(W\\\\) the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. \\\\(\\Delta = 1\\\\), or \\\\(\\Delta = 100\\\\)) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength \\\\(\\lambda\\\\)).\n",
    "\n",
    "\n",
    "#### SVM in OpenCV\n",
    "\n",
    "```python\n",
    "svm = cv2.ml.SVM_create()\n",
    "svm.setKernel(cv.ml.SVM_LINEAR)\n",
    "svm.setType(cv.ml.SVM_C_SVC)\n",
    "svm.train(trainData, cv.ml.ROW_SAMPLE, responses)\n",
    "svm.save('svm_data.dat')\n",
    "\n",
    "result = svm.predict(testData)[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax classifier\n",
    "\n",
    "It turns out that the SVM is one of two commonly seen classifiers. The other popular choice is the **Softmax classifier**, which has a different loss function. If you've heard of the binary Logistic Regression classifier before, the Softmax classifier is its generalization to multiple classes. Unlike the SVM which treats the outputs \\\\(f(x_i,W)\\\\) as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping \\\\(f(x_i; W) =  W x_i\\\\) stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the *hinge loss* with a **cross-entropy loss** that has the form:\n",
    "\n",
    "$$\n",
    "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\n",
    "$$\n",
    "\n",
    "where we are using the notation \\\\(f_j\\\\) to mean the j-th element of the vector of class scores \\\\(f\\\\). As before, the full loss for the dataset is the mean of \\\\(L_i\\\\) over all training examples together with a regularization term \\\\(R(W)\\\\). The function \\\\(f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}} \\\\) is called the **softmax function**: It takes a vector of arbitrary real-valued scores (in \\\\(z\\\\)) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you're seeing it for the first time but it is relatively easy to motivate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
