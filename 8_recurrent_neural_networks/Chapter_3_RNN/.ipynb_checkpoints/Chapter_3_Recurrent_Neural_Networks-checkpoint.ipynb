{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Introduction\n",
    "\n",
    "Hi guys! Hope you are doing well. As you know we are moving towards our image captioning application step by step. so far we have seen:\n",
    "\n",
    "1. What is Multimodal learning?\n",
    "2. How this works?\n",
    "3. Visual feature extraction using CNN.\n",
    "\n",
    "As we have already discussed Image captioning tasks involves 2 information modality i.e. Image and Text. Where:\n",
    "\n",
    "1. Image analysis which gives us information about content in the image. Such as what kind of objects present in the images or information about any landscape etc. \n",
    "\n",
    "2. Textual information processing helps to create a description of image based on image analysis.\n",
    "\n",
    "Now we can not perform text processing related jobs using simple feed forward network. The reason is pretty clear; text is a relative entity; Where one word combines with other word in a specific sequence such that it can make a proper sentence. for example. *\"A Dog is running across the beach\"* is a sentence with 7 words which are arranged in a specific order if we distort this order the sentence will not make any sense. I can not say *\"The beach is dog running\"*. \n",
    "\n",
    "Normal feed-forward neural networks are not capable to handle sequential information processing. For such tasks we need to implement networks which can memorize input information in a sequence and then can reproduce the information whenever required.\n",
    "\n",
    "Recurrent neural networks (RNN) are such kind of neural network family. Actually RNN are based on work of David Everett Rumelhart in 1986. David was an american psychologist who had many contribution in advancement of cognitive science. His work was carried away by Sepp Hochreiter and Jürgen Schmidhuber who created revolutionry Long short term memory networks (LSTM) in 1997. From then RNN have been used in many digital applications some are as advanced as speech to text conversion or vice versa, one language to another language translation, visual question answering, text generation etc. and for the same reason we will use RNN for generating captions for input images.\n",
    "\n",
    "In this chapter we will try to learn about:\n",
    "\n",
    "- What are RNNs?\n",
    "- How RNN works and learn?\n",
    "- How to create and use asimple RNN?\n",
    "\n",
    "So without wasting our time let's start with the discussion by introduction of RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Recurrent Neural Network (RNN)\n",
    "\n",
    "To understand about RNN following figure will help us. As you can see on your left hand side there is a normal feed forward neural network and an RNN on the right. Now As we have discussed earlier in the chapter that RNN has a provision of memorize the sequence thats why it is the best choice for time series analysis. Now how they memorize sequences? Let's understand how. \n",
    "\n",
    "Feed Forward network | Recurrent Neural Network\n",
    "- | - \n",
    "![alt](1_FFANN.png) | ![alt](2_RNN.png)\n",
    "\n",
    "When it makes a decision it not only utilize information from current input but it also uses information learned by previous input as well. It uses feedback loops to make this possible. As you can see in following figure (right side) each neuron has a feedback loop which revert back information learned in past so this neurons will have to process multiple information as a sequence each time; which makes possible to learn sequential information for them. \n",
    "\n",
    "Therefore a Recurrent Neural Network has two inputs, one is present and other is recent past. This arrangement is very important because the sequence of data (mainly time series) contains crucial information about what is coming next, and this is why a RNN can do those tasks which other networks can’t do. A typical RNN unit looks like following;\n",
    "\n",
    "<img src='3_RNN.png' width='150'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 How RNN learns?\n",
    "\n",
    "Well I hope you got the point that how RNN learns the sequences by utilizing multiple informations (present and past inputs) at the same time. But do you wonder how exactly this happens. In this section we will try to learn the same.\n",
    "\n",
    "So as RNN is also a neural network it learns in the same way as all other neural networks. **Gradient Decent and Backpropagation**. Whether it is a simple feed forward neural net or a deep convolutional network all learns (basically optimize weights) using these two magical algorithms. You will learn about both of the algorithm in neural network course work. But to understand working of RNN we need to dicuss about these two in brief. \n",
    "\n",
    "Gradient tells us about the change or differences between two quantities *honestly I feel this is just a fancy term for subtraction*. But on a serious note gradients are partial derivatives of output error (between actual output vs predicted output) with respect to weights. Now to do this operation for each layer we need to propagate the error in reverse direction through the network and this is what the backpropagation does. Following figure will give you an idea of the process.\n",
    "\n",
    "<img src='4_ForwardPass.png' width='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Backpropagation and Gradient decent\n",
    "\n",
    "So what actually happening in above figure? We can understand this in two steps:\n",
    "\n",
    "1. Forward pass.\n",
    "2. Backward pass.\n",
    "\n",
    "During **forward pass** our neural network takes input ($I_1,I_2$ and $I_3$) from first layer (Input layer) and pass it down  to hidden layer using weight connections. In hidden layer network performs sum of product between weights ($w$) and inputs ($I$) and applies an activation function ($\\sigma$) to the result. Then this activated signal will pass down to the output layer with the help of another set of weight connections. At output layer once again network performs sum of product between activated outputs (from hidden layer) and weights. At the end our network generates predictions ($P$) with the help of appropriate activation (sigmoid, softmax etc.) function. \n",
    "\n",
    "In the **backward pass**; we calculate the difference between actual output ($O$) and predicted output ($P$) this difference is known as error ($E$) generated by the network. We will use this error to optimize the network weights using gradient decent algorithm. We will pass the error back through the same connections as we have came through during forward pass. This time we will try to find out partial derivatives of the error with respect to each weight (in each layer). This calculation tells us the amount of change required in the respected weight quantity to minimize the error. Then we just add the partial derivative to the original weight. This operation known as **gradient decent** and the backward pass known as **backpropagation** algorithm.\n",
    "\n",
    "We can tune any weight present at any layer in the network using above algorithm this process is what known as neural network training (or optimization). Backpropagation is used to train any kind of neural network whether it is a CNN or a RNN. For RNN this algorithm works in a different way. Let's see how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 RNN unrolling and Backpropagation through time (BPTT)\n",
    "\n",
    "In an RNN time step is most important unit it helps our network to learn sequences. Time steps also play crucial role during network's weight optimization (training) using backpropagation thus the algorithm known as backpropagation through time or BPTT. To understand how exactly this happens we need to unroll our recurrent network.\n",
    "\n",
    "We will try to learn about BPTT after learning about RNN unrolling. Following figure will give you an idea about RNN unrolling.\n",
    "\n",
    "<img src='5_BPTT.png' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What the hell is this man!* I know this is what came in your mind when you saw above image. Well let's try to understand what is happening here. As you can see in your left side a single RNN unit has shown. Here you can see one input layer ($X$), one hidden layer ($h$) and one output layer ($O$) of the network with 3 different variables i.e Inputs $U$, Weights $W$ and output $V$. \n",
    "\n",
    "So we fed input ($U$) from input layer ($X$) in addition of previous output ($V$) to the network. This information will be processed through hidden layer ($h$) using sum of products from weights ($W$) and it generates output ($V$) from output layer ($O$). This all happens in cycles. Oh man its so complicated! we need to simplyfy it.     \n",
    "\n",
    "We can assume an RNN as a Sequence of plain neural networks as shown in left side of above figure. One network for each time step i.e. one network which has processed information in past at time step $t-1$, one which will process present information at time step $t$ and one will process information at $t+1$ time step. This process is known as **unrolling of an RNN**. \n",
    "\n",
    "Let's see what happens during first cycle of foward pass. Input to the network of present time step $t$ will be $X_t$ and $V_{t-1}$ (This was generated by network at $t-1$) this set of input will be processed at $h_t$ and network will generate an output $V_t$ now this output along with input $X_{t+1}$ will fed to the network at time step $t+1$. Output at $O_{t+1}$ will have information processed by network $t-1$ and network $t$. Now we will understand this process with the help of an example.\n",
    "\n",
    "Suppose we want to generate a sentence *\"Running with a dog\"* from our RNN. For first time step as there will not be any previous output available we will feed a constant word $\"startseq\"$ as output of $t-1$. And we will also add a word $\"endseq\"$ to the sentence so that our network can know when to stop processing.\n",
    "\n",
    "So our input sentence will now look something like this:\n",
    "\n",
    "*\"stratseq Running with dog endseq\"*\n",
    "\n",
    "Now for each time step following will happen.\n",
    "\n",
    "            Network time step     input (X(t))                               output (O(t))\n",
    "            \n",
    "                    t            startseq                                     Running\n",
    "                    t+1          startseq Running                             with\n",
    "                    t+2          startseq Running with                        a                   \n",
    "                    t+3          startseq Running with a                      Dog\n",
    "                    t+4          startseq Running with a Dog endseq     \n",
    "       \n",
    "So this is how our RNN will learn the sentence. As you can see at each time step our RNN is taking input from all previous steps. This helps our network in learning sequences. We can understand above process from the following figure. \n",
    "\n",
    "<img src='6_RNN_Unroll.png' width='300'>\n",
    "\n",
    "As you can see in above figure every time step have all the previous outputs as the input.  \n",
    "\n",
    "Now If when we do Backpropagation Through Time, it is done in the same way of unrolling, as the error of a given timestep depends on the previous timestep.\n",
    "\n",
    "In BPTT the error is back-propagated from the last time step to the first time step, while unrolling all the time steps. This allows network to calculate the error for every timestep, this allows updating the weights. Because of computation requiredat each time step; BPTT could be computationally expensive when you have a higher number of timesteps.\n",
    "\n",
    "Now as we have understand about basic concepts behind the working of RNN. This is the time to jump into some implementation of learned concepts.\n",
    "\n",
    "## 3.3.3 Implementation on RNN\n",
    "\n",
    "Let's try to solve a simple problem of sentiment analysis using a very basic RNN. Before going ahead you should know about the sentiment analysis.\n",
    "\n",
    "Sentiment analysis is one of the widely known application of natural language processing (NLP). It dealt with quantifying a statement (or a sentence) into negative or positive category. Its like a movie review which could be positive or negative. We will create a small (toy) data set of 4 reviews to learn about the sentiments behind the them. following 4 reviews will be our data set.\n",
    "\n",
    "- Movie is good and not a waste\n",
    "- Movie is a waste and not good\n",
    "- Movie is not a waste and good\n",
    "- Movie is not good and a waste\n",
    "\n",
    "This example is constructed so that the only difference between the reviews is the order of the words. Now we will create a data set consist of these 4 reviews with their corresponding labels. We will choose 1 for Positive review and 0 for negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['movie is good and not a waste',\n",
    "             'movie is a waste and not good',\n",
    "             'movie is not a waste and good',\n",
    "             'movie is not good and a waste']\n",
    "\n",
    "sentiments = [1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with building a vocabulary of the words in our data set. This is an important step here we will assign a unique number to each word. It will convert this character data set into numerical which will be a suitable input format for neural networks. We will use $CountVectorizer()$ class from sklearn library. so let's import and apply to our data set. We will avoid articles and single character for the time to reduce the complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0, 'good': 1, 'is': 2, 'movie': 3, 'not': 4, 'waste': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following line will import the library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Now let's build the vocab.\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences) # build the vocabulary\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks cool! As you can see each word has assigned with a unique number in a dictionary form. This dictionary will work as a look up table for us. So when we want to convert network predictions into sentences; we will use the same vocab to do so.\n",
    "\n",
    "In next step we will need to use this vocab to convert sentences into numerical sequences. We will write a small method $CreateSequeces()$ for this task. Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use numpy for matrics related operations\n",
    "import numpy as np\n",
    "def CreateSequences(vec, corpus):\n",
    "    \n",
    "    #VEC: is vocab.\n",
    "    #CORPUS: is data set\n",
    "    \n",
    "    # We will create a tokenizer to convert the sentence into numerics\n",
    "    tokenizer = vec.build_analyzer()\n",
    "    \n",
    "    # return a sequence of tokens that appear in the\n",
    "    # vocabulary for each sentence\n",
    "    # ignores tokens that are not found in the vocabulary\n",
    "    # may return an empty list if no tokens are found\n",
    "    # in the dictionary\n",
    "    return np.array([\n",
    "        [vec.vocabulary_[token]\n",
    "         for token in tokenizer(doc)\n",
    "         if token in vec.vocabulary_\n",
    "        ]\n",
    "        for doc in corpus\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code method will help us to convert sequences into integer. let's see what it does exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 1, 0, 4, 5],\n",
       "       [3, 2, 5, 0, 4, 1],\n",
       "       [3, 2, 4, 5, 0, 1],\n",
       "       [3, 2, 4, 1, 0, 5]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_seq = CreateSequences(vectorizer,sentences)\n",
    "numeric_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good I think.\n",
    "\n",
    "Now we will move ahead and write a method to implement a small RNN. We will use keras (with tensorflow backend) for implementing this network. Our network will have following architecture:\n",
    "\n",
    "- Embedding layer: which convert input word vectors into dense representation.\n",
    "- RNN layer: Which is actually going to learn the sequence. we will use 5 neurons in the layer.\n",
    "- Dense layer with sigmoid activation: It will generate sigmoid probabilties at the output of the network.\n",
    "\n",
    "Now before going further into discussion let's see what is the significant of the embedding layer in our network.\n",
    "\n",
    "#### Word Embedding\n",
    "\n",
    "In brief; a word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
    "It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
    "\n",
    "Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. The position of a word in the learned vector space is referred to as its embedding.\n",
    "\n",
    "Two popular examples of methods of learning word embeddings from text include:\n",
    "\n",
    "-\tWord2Vec.\n",
    "-\tGloVe.\n",
    "\n",
    "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.\n",
    "\n",
    "We will use Keras word embedding layer, let’s see how we can use it;\n",
    "\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "-\tIt can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "-\tIt can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "-\tIt can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "-\tinput_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "-\toutput_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "-\tinput_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
    "\n",
    "I think it is enough to discuss about word embedding. Now let us again dive into our problem.\n",
    "\n",
    "Apart from above discussed architecture; we will use binary_crossentropy as a loss fucntion to measure the performance of the network. We will train this network using 'adam' optimizer for 10 epochs. \n",
    "\n",
    "Let's implement these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import sequential model class\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Network layers\n",
    "from keras.layers import Embedding,SimpleRNN,Dense\n",
    "\n",
    "def RNN(vectorizer,numeric_seq,embedding_dim):\n",
    "    # Our model will be sequential\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Embedding layer for dense representation of words\n",
    "    model.add(Embedding(len(vectorizer.vocabulary_),\n",
    "                        embedding_dim,\n",
    "                        input_length=len(numeric_seq[0])))\n",
    "    \n",
    "    #Here is our simple RNN with 5 units\n",
    "    model.add(SimpleRNN(5))\n",
    "    \n",
    "    #Final layer to generate predictions\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the model using above defination and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 6, 300)            1800      \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 5)                 1530      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 3,336\n",
      "Trainable params: 3,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.7164 - acc: 0.5000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7058 - acc: 0.2500\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6817 - acc: 0.5000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6697 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6577 - acc: 0.7500\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6460 - acc: 0.7500\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6343 - acc: 0.7500\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6226 - acc: 0.7500\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6111 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb4ff970c88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(vectorizer,numeric_seq,300)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(numeric_seq, np.array(sentiments), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh WOW! our network got a perfect fit on the training data in just 6 epochs. This is due to our sequences are extremly small. Well we should not get happy untill we test the performance of our model on unseen data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44551092]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_review = 'a waste and is not a good movie'\n",
    "\n",
    "new_review_seq = [vectorizer.vocabulary_[token]\n",
    "                  for token in new_review.split() \n",
    "                  if token in vectorizer.vocabulary_\n",
    "                 ]\n",
    "\n",
    "model.predict(np.array([new_review_seq]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well probability seems to pointing towards class 0 which is the negative one. and I think our network did a pretty good job here. and congratulations to you to guys on making of your first ever recurrent neural network. dont you think its simple? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all looks good up till here. Now simple RNN architecture is good choice over feed forward network to learn sequences which are smaller in size. Simple RNN have very less scope to memorize large sequeces for larger time steps (such as memorizing a paragraph). To solve this problem of learning larger sequences we will use networks which have capability of memorize larger sequences. We will talk about following two networks for this task:\n",
    "\n",
    "1. Long short term memory (LSTM) networks.\n",
    "2. Gated recurrent units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 LSTM Networks\n",
    "\n",
    "LSTMs are well suited network for learning larger sequences because it contain memory element in the architecture. This enables read, write and delete ability to the network. Following is the architecture of a typical LSTM unit.\n",
    "\n",
    "<img src='7_The_LSTM_cell.png' width='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! I know its nothing like what you have expected. So this is what a typical LSTM unit. Let's try to understand it.\n",
    "\n",
    "- Unit has three inputs here; $X_t$ is present input. $h_{t-1}$ is output of previous time step. One new thing here is $C_{t-1}$ which is memory of previous step. This memory has a crucial role in learning sequences. we will see Why? later.\n",
    "- Unit has 3 sigmoid ($\\sigma$) units (similar like activation function). It also has two hyper tangent ($tanh$) units.\n",
    "- This unit has 2 outputs i.e. present output $h_t$ and present time step memory $C_t$.\n",
    "\n",
    "So we got all of the elements. Now how do they work? \n",
    "\n",
    "Three sigmoid units are shown in the figure are work as switches or gates. As you know a sigmoid function generates response in the range of 0 and 1. So 0  means the gate is $OFF$ and 1 means gate is fully open. In this way network controls the amount of information to be passed.\n",
    "\n",
    "LSTM has mainly three essential gates; Input gate, Forget get and output gate.\n",
    "\n",
    "**Input gate** will recieve input for present time step, output of previous time step and memory of previous time step. So inputs $X_t$ and $h_{t-1}$ get combined and processed through first sigmoid unit. This will generate an output in the range of 0 to 1.\n",
    "\n",
    "This output will be multiplied by the memory $C_{t-1}$. This operation will work as **forget gate**. If output of first sigmoid unit is 0 then previous memory will be erased from the network. So higher the output of sigmoid unit more memory will be used. It will help network to learn only significant length of sequences from the data set.\n",
    "\n",
    "For a moment we will forget about the second sigmoid unit and move ahead to the $tanh$ function. This is the place where new memory will be created for current time step. Our network will learn the current sequence here and process it for further usage. Now the role of second sigmoid unit comes into the picture. It will again generate an output in the range of 0 to 1 and will control the amount of new memory (using multiplication) to be added to the older memory. After this operation our unit is now ready with the updated memory $C_t$ for current time step.\n",
    "\n",
    "Now is the role of output gate comes into the picture. Output $h_t$ will be generated using final $tanh$ activation and amount of information will be decided by the sigmoid unit. Third and last sigmoid unit in the architecture will decide how much output is significant to flow further from the current time step. \n",
    "\n",
    "Oh man this was all so complicated. But I hope you got my point. There is a beutiful explanation of the whole operation is given by Shi Yan on his medium most. you can check it out here at [Understanding LSTM and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714). Shi describe the whole process using colorful diagrams. It will help you to learn the gist pretty quickly. Go and checkout the post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Text generation using LSTM\n",
    "\n",
    "In this section we will implement a LSTM network using keras and tensorflow. We will use this network for text generation. We will take advantage of LSTM's capability to learn large sequences. We will first present a whole book to our network to learn the significant sequences from it. Then we will use the same network to generate some text which will look alike the writing style of original author. Seem interesting! Let's jump straight to the problem.\n",
    "\n",
    "### 3.5.1 Data set\n",
    "\n",
    "We will use the popular chid story book *\"Alice in the wonderland by lewis carol\"* for our task. There are two reasons to pick this book. First the size of book is really small so vocabulary will be small for our network to learn. Second it is free to download and use. This book is available on [Project Gutenberg](https://www.gutenberg.org/ebooks/11) you can download it and keep it in the project directory. We will download it in plain text format.\n",
    "\n",
    "### 3.5.2 Text pre-processing\n",
    "\n",
    "Now we can not use the book text as it is. As the book has more than 150000 charecters in it we need to concise our vocabulary. we will convert all upper case characters to lower case in our data set. One more important thing we need to understand is that we will work on the problem of single charecter generation not the whole word generation.\n",
    "\n",
    "We will start with loading of our data  set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data set\n",
    "filename = \"AliceInTheWonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to create mapping of charecters to the numbers so that our neural network can process it. lets do this in following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'j': 39, 'q': 46, '“': 58, '5': 18, '_': 29, 'n': 43, '[': 27, 'r': 47, '/': 12, 'y': 54, '#': 3, '\\n': 0, 'e': 34, 'u': 50, '”': 59, ';': 24, '7': 20, 'g': 36, '0': 13, '‘': 56, 'o': 44, 'h': 37, 'w': 52, 'v': 51, '’': 57, ',': 9, 'k': 40, 'c': 32, '9': 22, 'x': 53, 't': 49, '(': 6, '@': 26, 'd': 33, ' ': 1, '!': 2, '4': 17, '%': 5, 'm': 42, 'a': 30, ')': 7, '*': 8, 'f': 35, ':': 23, '8': 21, 'p': 45, 'l': 41, '$': 4, ']': 28, 's': 48, '-': 10, '.': 11, 'i': 38, '3': 16, '2': 15, 'z': 55, '?': 25, 'b': 31, '1': 14, '6': 19}\n"
     ]
    }
   ],
   "source": [
    "# Create charecter mappings\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see these are the unique charecters and there integer mapping which present in our book. Now as you can see we are having around 60 charecters left into our vocab. our next task is to prepare training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Training data preparation\n",
    "\n",
    "We will split our whole data set in sequences of constent length and try to learn these sequences. Our sequence length will be 100 charecters. Let's see what i mean by fixeld length sequences.\n",
    "\n",
    "Suppose we have a fixed sequence length of 5 and we want to learn  word TRAINING. Our Input and output for first three sequences will look like this.\n",
    "\n",
    "       Input      Output\n",
    "    1. TRAIN       I\n",
    "    2. RAINI       N\n",
    "    3. AININ       G\n",
    "\n",
    "Following lines of code will do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163815\n",
      "Total Vocab:  60\n",
      "Total Patterns:  163715\n",
      "[45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34] 1\n",
      "[47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34, 57] 30\n",
      "[44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34, 57, 48] 43\n",
      "[39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34, 57, 48, 1] 54\n",
      "[34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34, 57, 48, 1, 30] 44\n"
     ]
    }
   ],
   "source": [
    "# Lets see our charecter size and Vocab size\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# Here we will define the sequence length\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# Lets create the sequence\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# Lets print a few sequences upto length 25 and their output\n",
    "for i in range(5):\n",
    "    print(dataX[i][0:25],dataY[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use softmax probabilty at the output layer for which we need to convert our output label into catogorical variable. We will also apply normalization on input data  set for faster convergence of the netwirk. We will do this in following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use numpy for matrix based calculations\n",
    "import numpy as np\n",
    "\n",
    "# We will use keras for converting output variable to categorical \n",
    "from keras.utils import np_utils\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4  Network\n",
    "\n",
    "Its the time to define our LSTM network. We will implement following architecture.\n",
    "\n",
    "- We will use keras sequential model for implementation of the architecture.\n",
    "- There will be two LSTM layers with 256 units.\n",
    "- We will use dropout regularization after each LSTM layer to avoid overfitting of the network.\n",
    "- Finally we will use a dense layer with sofmax activation to generate output probabilties.\n",
    "\n",
    "Apart from that we will use *categorical_crossentropy* as the loss function which will try to optimize using *adam* optimizer.\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5 Network Training\n",
    "\n",
    "All looks well and good and now we are ready to train our network. We will use keras' $ModelCheckpoint$ to store the best model for lowest loss with defined path. Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# define the checkpoint\n",
    "filename = \"weights-improvement-09-2.2497.hdf5\"\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "model.load_weights(filename)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our model for 50 epochs with batchsize of 256. Let's do it in the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 2.1865\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.18652, saving model to weights-improvement-01-2.1865.hdf5\n",
      "Epoch 2/50\n",
      "163715/163715 [==============================] - 912s 6ms/step - loss: 2.1345\n",
      "\n",
      "Epoch 00002: loss improved from 2.18652 to 2.13450, saving model to weights-improvement-02-2.1345.hdf5\n",
      "Epoch 3/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 2.0870\n",
      "\n",
      "Epoch 00003: loss improved from 2.13450 to 2.08701, saving model to weights-improvement-03-2.0870.hdf5\n",
      "Epoch 4/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 2.0433\n",
      "\n",
      "Epoch 00004: loss improved from 2.08701 to 2.04332, saving model to weights-improvement-04-2.0433.hdf5\n",
      "Epoch 5/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.9994\n",
      "\n",
      "Epoch 00005: loss improved from 2.04332 to 1.99941, saving model to weights-improvement-05-1.9994.hdf5\n",
      "Epoch 6/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.9607\n",
      "\n",
      "Epoch 00006: loss improved from 1.99941 to 1.96070, saving model to weights-improvement-06-1.9607.hdf5\n",
      "Epoch 7/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.9249\n",
      "\n",
      "Epoch 00007: loss improved from 1.96070 to 1.92488, saving model to weights-improvement-07-1.9249.hdf5\n",
      "Epoch 8/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 1.8923\n",
      "\n",
      "Epoch 00008: loss improved from 1.92488 to 1.89226, saving model to weights-improvement-08-1.8923.hdf5\n",
      "Epoch 9/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.8553\n",
      "\n",
      "Epoch 00009: loss improved from 1.89226 to 1.85533, saving model to weights-improvement-09-1.8553.hdf5\n",
      "Epoch 10/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.8266\n",
      "\n",
      "Epoch 00010: loss improved from 1.85533 to 1.82661, saving model to weights-improvement-10-1.8266.hdf5\n",
      "Epoch 11/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.7971\n",
      "\n",
      "Epoch 00011: loss improved from 1.82661 to 1.79707, saving model to weights-improvement-11-1.7971.hdf5\n",
      "Epoch 12/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.7621\n",
      "\n",
      "Epoch 00012: loss improved from 1.79707 to 1.76211, saving model to weights-improvement-12-1.7621.hdf5\n",
      "Epoch 13/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.7344\n",
      "\n",
      "Epoch 00013: loss improved from 1.76211 to 1.73443, saving model to weights-improvement-13-1.7344.hdf5\n",
      "Epoch 14/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.7105\n",
      "\n",
      "Epoch 00014: loss improved from 1.73443 to 1.71053, saving model to weights-improvement-14-1.7105.hdf5\n",
      "Epoch 15/50\n",
      "163715/163715 [==============================] - 913s 6ms/step - loss: 1.6849\n",
      "\n",
      "Epoch 00015: loss improved from 1.71053 to 1.68485, saving model to weights-improvement-15-1.6849.hdf5\n",
      "Epoch 16/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.6574\n",
      "\n",
      "Epoch 00016: loss improved from 1.68485 to 1.65738, saving model to weights-improvement-16-1.6574.hdf5\n",
      "Epoch 17/50\n",
      "163715/163715 [==============================] - 912s 6ms/step - loss: 1.6343\n",
      "\n",
      "Epoch 00017: loss improved from 1.65738 to 1.63431, saving model to weights-improvement-17-1.6343.hdf5\n",
      "Epoch 18/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 1.6110\n",
      "\n",
      "Epoch 00018: loss improved from 1.63431 to 1.61101, saving model to weights-improvement-18-1.6110.hdf5\n",
      "Epoch 19/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.5831\n",
      "\n",
      "Epoch 00019: loss improved from 1.61101 to 1.58311, saving model to weights-improvement-19-1.5831.hdf5\n",
      "Epoch 20/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 1.5614\n",
      "\n",
      "Epoch 00020: loss improved from 1.58311 to 1.56139, saving model to weights-improvement-20-1.5614.hdf5\n",
      "Epoch 21/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 1.5442\n",
      "\n",
      "Epoch 00021: loss improved from 1.56139 to 1.54420, saving model to weights-improvement-21-1.5442.hdf5\n",
      "Epoch 22/50\n",
      "163715/163715 [==============================] - 912s 6ms/step - loss: 1.5276\n",
      "\n",
      "Epoch 00022: loss improved from 1.54420 to 1.52762, saving model to weights-improvement-22-1.5276.hdf5\n",
      "Epoch 23/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.5040\n",
      "\n",
      "Epoch 00023: loss improved from 1.52762 to 1.50396, saving model to weights-improvement-23-1.5040.hdf5\n",
      "Epoch 24/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 1.4835\n",
      "\n",
      "Epoch 00024: loss improved from 1.50396 to 1.48345, saving model to weights-improvement-24-1.4835.hdf5\n",
      "Epoch 25/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 1.4679\n",
      "\n",
      "Epoch 00025: loss improved from 1.48345 to 1.46791, saving model to weights-improvement-25-1.4679.hdf5\n",
      "Epoch 26/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.4471\n",
      "\n",
      "Epoch 00026: loss improved from 1.46791 to 1.44713, saving model to weights-improvement-26-1.4471.hdf5\n",
      "Epoch 27/50\n",
      "163715/163715 [==============================] - 919s 6ms/step - loss: 1.4353\n",
      "\n",
      "Epoch 00027: loss improved from 1.44713 to 1.43529, saving model to weights-improvement-27-1.4353.hdf5\n",
      "Epoch 28/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.4179\n",
      "\n",
      "Epoch 00028: loss improved from 1.43529 to 1.41790, saving model to weights-improvement-28-1.4179.hdf5\n",
      "Epoch 29/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.4000\n",
      "\n",
      "Epoch 00029: loss improved from 1.41790 to 1.40002, saving model to weights-improvement-29-1.4000.hdf5\n",
      "Epoch 30/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.3860\n",
      "\n",
      "Epoch 00030: loss improved from 1.40002 to 1.38595, saving model to weights-improvement-30-1.3860.hdf5\n",
      "Epoch 31/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.3686\n",
      "\n",
      "Epoch 00031: loss improved from 1.38595 to 1.36865, saving model to weights-improvement-31-1.3686.hdf5\n",
      "Epoch 32/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.3583\n",
      "\n",
      "Epoch 00032: loss improved from 1.36865 to 1.35826, saving model to weights-improvement-32-1.3583.hdf5\n",
      "Epoch 33/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.3436\n",
      "\n",
      "Epoch 00033: loss improved from 1.35826 to 1.34364, saving model to weights-improvement-33-1.3436.hdf5\n",
      "Epoch 34/50\n",
      "163715/163715 [==============================] - 915s 6ms/step - loss: 1.3311\n",
      "\n",
      "Epoch 00034: loss improved from 1.34364 to 1.33109, saving model to weights-improvement-34-1.3311.hdf5\n",
      "Epoch 35/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.3139\n",
      "\n",
      "Epoch 00035: loss improved from 1.33109 to 1.31385, saving model to weights-improvement-35-1.3139.hdf5\n",
      "Epoch 36/50\n",
      "163715/163715 [==============================] - 918s 6ms/step - loss: 1.3080\n",
      "\n",
      "Epoch 00036: loss improved from 1.31385 to 1.30803, saving model to weights-improvement-36-1.3080.hdf5\n",
      "Epoch 37/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.2910\n",
      "\n",
      "Epoch 00037: loss improved from 1.30803 to 1.29099, saving model to weights-improvement-37-1.2910.hdf5\n",
      "Epoch 38/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.2824\n",
      "\n",
      "Epoch 00038: loss improved from 1.29099 to 1.28243, saving model to weights-improvement-38-1.2824.hdf5\n",
      "Epoch 39/50\n",
      "163715/163715 [==============================] - 918s 6ms/step - loss: 1.2670\n",
      "\n",
      "Epoch 00039: loss improved from 1.28243 to 1.26697, saving model to weights-improvement-39-1.2670.hdf5\n",
      "Epoch 40/50\n",
      "163715/163715 [==============================] - 914s 6ms/step - loss: 1.2631\n",
      "\n",
      "Epoch 00040: loss improved from 1.26697 to 1.26308, saving model to weights-improvement-40-1.2631.hdf5\n",
      "Epoch 41/50\n",
      "163715/163715 [==============================] - 917s 6ms/step - loss: 1.2503\n",
      "\n",
      "Epoch 00041: loss improved from 1.26308 to 1.25034, saving model to weights-improvement-41-1.2503.hdf5\n",
      "Epoch 42/50\n",
      "163715/163715 [==============================] - 919s 6ms/step - loss: 1.2363\n",
      "\n",
      "Epoch 00042: loss improved from 1.25034 to 1.23635, saving model to weights-improvement-42-1.2363.hdf5\n",
      "Epoch 43/50\n",
      "163715/163715 [==============================] - 918s 6ms/step - loss: 1.2273\n",
      "\n",
      "Epoch 00043: loss improved from 1.23635 to 1.22728, saving model to weights-improvement-43-1.2273.hdf5\n",
      "Epoch 44/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.2196\n",
      "\n",
      "Epoch 00044: loss improved from 1.22728 to 1.21965, saving model to weights-improvement-44-1.2196.hdf5\n",
      "Epoch 45/50\n",
      "163715/163715 [==============================] - 918s 6ms/step - loss: 1.2080\n",
      "\n",
      "Epoch 00045: loss improved from 1.21965 to 1.20803, saving model to weights-improvement-45-1.2080.hdf5\n",
      "Epoch 46/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.2005\n",
      "\n",
      "Epoch 00046: loss improved from 1.20803 to 1.20045, saving model to weights-improvement-46-1.2005.hdf5\n",
      "Epoch 47/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.1894\n",
      "\n",
      "Epoch 00047: loss improved from 1.20045 to 1.18937, saving model to weights-improvement-47-1.1894.hdf5\n",
      "Epoch 48/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.1805\n",
      "\n",
      "Epoch 00048: loss improved from 1.18937 to 1.18047, saving model to weights-improvement-48-1.1805.hdf5\n",
      "Epoch 49/50\n",
      "163715/163715 [==============================] - 913s 6ms/step - loss: 1.1757\n",
      "\n",
      "Epoch 00049: loss improved from 1.18047 to 1.17572, saving model to weights-improvement-49-1.1757.hdf5\n",
      "Epoch 50/50\n",
      "163715/163715 [==============================] - 916s 6ms/step - loss: 1.1667\n",
      "\n",
      "Epoch 00050: loss improved from 1.17572 to 1.16666, saving model to weights-improvement-50-1.1667.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x65b7d82fd0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=512, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network will take lot of time to train on normal CPU. You will need to have a GPU card for faster training or else it would be difficult for you to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.6 Test our Network\n",
    "\n",
    "To generate text from our trained network is quite simple. We need to load our model give it some initial seed charecters to start generating predictions. As our network will give us numerical outputs we will need to convert it back to charecters. We will do this with reverse mapping. We will do it in following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-50-1.1667.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dictionary for reverse mapping of numbers into charecters. We will do it in following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In following lines we will create a seed point for input to our network and generate text out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  somebody to talk to.’\n",
      "\n",
      "‘how are you getting on?’ said the cat, as soon as there was mouth\n",
      "enough fo \"\n",
      "r it as she was as all the wiile say the was silint and looked at the caterpillar with she were sialed teree of the little goowessetion. \n",
      "the cormouse should she had getting them she was so food to the steen and the other pides her fend and then a little blt of the was siating about at the cool, and she went on againseng to her in the distance, and she went on againseng to her eno a lore ruestion, and the queen said not and the thing, she had not as the could not aeak and coower, and she was quite sllesced at the wiite rabbit as the was splatiing about in the was so tasing a little bot of the was siating about at the cool, and she went on againseng to her in the distance, and she went on againseng to her eno a lore ruestion, and the queen said not and the thing, she had not as the could not aeak and coower, and she was quite sllesced at the wiite rabbit as the was splatiing about in the was so tasing a little bot of the was siating about at the cool, and she went on againseng to her in\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** *Running this example first outputs the selected random seed, and then each character as it is generated.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! it looks like our network is in action. Well there are too much grammatical mistakes and many non english words in the prediction. But this is what you can expect from a 2 layer LSTM network. You can see many of the words are correct. We can certainly improve this accuracy by increasing the number of LSTM units as well as layers. But for the time being these results are enough to show the capability of our network.\n",
    "\n",
    "In next section we will learn about one more flavour of RNN; Gated Recurrent Units or GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Gated Recurrent Units\n",
    "\n",
    "We have just finished with LSTM and seen their importance in sequential learning. But if you have noticed training spped of the network you can clearly say; *\"It is too slow man\"*. GRU could be speedier than LSTM as they have slightly different architecture than LSTM. In this section we will learn about GRU in brief.\n",
    "\n",
    "GRU has almost similar architecture and working to LSTM. It has some minor changes in gate architecture. As we have seen in case of LSTM there were three gates which were used to flow only significant information from the network. Here in GRU there are only two gates which have similar kind of operation to LSTM.\n",
    "\n",
    "1. Update gate.\n",
    "2. Reset Gate.\n",
    "\n",
    "Following image shows a typical architecture of a GRU.\n",
    "\n",
    "<img src='8_GRU_1.png' width='500'>\n",
    "image source: [Medium-GRU](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above architecture as you can see; there are two sigmoid ($\\sigma$) units and one $tanh$ unit. $X_t$ and $h_t$ are current input and output respectivly. $h_{t-1}$ is outpuy of previous step. Significance of sigmoid and $tanh$ is same as we have seen in LSTM. Sigmoid will be used to control the flow of how much information should be passed through the network and $tanh$ will be responsible to generate information itself. If you notice there is no provision for previous or current memory. This is because there is no seperate connection has made for the memory. input from the past step i.e. $h_{t-1}$ will be treated as the previous step memory element in here and similarly $h_t$ will be treated as current step memory element.\n",
    "\n",
    "**Update Gate**: Working of update gate is exactly similar to the forget gate in an LSTM unit. It is responsible to control how much information from previous time step should be flow through the current time step. Again sigmoid unit will be used to decide the amount of information. This gate will recieve combined information from $h_t-1$ and $X_t$. This information will be passed through first sigmoid unit (in green color) and then multiplied with the original information ($h_{t-l}$) from previous time step. This is how network decide how much it need to remember.\n",
    "\n",
    "we can summarize the action in the following equation:\n",
    "\n",
    "\\begin{align}\n",
    "\\ z_t = \\sigma(WX_t+Uh_{t-1}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Here $W$ and $U$ are corresponding weight connections. So you can see how sigmoid can affect the information to be passed to memory unit $tanh$.\n",
    "\n",
    "**Reset Gate**: Now second sigmoid unit creates here the reset gate. If you will see the figure above you can clearly understand that output of second sigmoid unit is similar to the output of previous sigmoid unit. *So what the hell is the difference here?*\n",
    "\n",
    "\\begin{align}\n",
    "\\ r_t = \\sigma(WX_t+Uh_{t-1}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Well the difference will come into the picture when we will discuss about the functionality of current memory element $tanh$.\n",
    "\n",
    "**Current Memory Content**: The responsibility of creating information for current time step is of $tanh$ function. let's first see the equation for this.\n",
    "\n",
    "\\begin{align}\n",
    "\\ h_t = tanh(W*X_t+r_t*Uh_{t-1}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Here $*$ shows the hadamard product between two vectors.\n",
    "\n",
    "So you can see that current time step output is directly dependent on output from second sigmoid unit ($r_t$). This will help our network to forgot the amount of the information to be processed at the output.\n",
    "\n",
    "Now as you can see in the figure this is not it. We will further manipulate the memory content. lets discuss how.\n",
    "\n",
    "**Final Memory Content**: Now as you can see in figure after passing the information from $tanh$ we will further multiply it with update gate information which will tell us how much information to be passed out for the current time step. Update gate will also helps us to decide what to remember from previous time step information ($h_{t-1}$). We can write an equation for final output of the current time step.\n",
    "\n",
    "\\begin{align}\n",
    "\\ h_t = z_t*h_{t-1}+(1-z_t)*h_t \\\\\n",
    "\\end{align}\n",
    "\n",
    "$h_t$ used in right side of the equation is information processed before the final output.\n",
    "\n",
    "So that is it! We have learned basic concept lying in GRU and seen how it different than LSTM. Although it is quite difficult to choose which one is better and it is still an area of research by the engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "So we have learned quite a lot in this chapter. We have almost cover every aspect of the recurrent neural networks right from theory to practical implementation. In next chapter we will use all this information to generate captions for our images! Did you forgot that! Well let's meet in the next chapter. Till than happy learning guys :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quiz\n",
    "\n",
    "## Q1. What are the steps for using a gradient descent algorithm?\n",
    "\n",
    " 1. Calculate error between the actual value and the predicted value\n",
    " 2. Reiterate until you find the best weights of network\n",
    " 3. Pass an input through the network and get values from output layer\n",
    " 4. Initialize random weight and bias\n",
    " 5. Go to each neurons which contributes to the error and change its respective values to reduce the error\n",
    "\n",
    "- a) 1, 2, 3, 4, 5\n",
    "- b) 5, 4, 3, 2, 1\n",
    "- c) 3, 2, 1, 5, 4\n",
    "- d) 4, 3, 1, 5, 2\n",
    "\n",
    "Ans: (D)\n",
    "\n",
    "## Q2. In which neural net architecture, does weight sharing occur?\n",
    "\n",
    "- a) Convolutional neural Network\n",
    "- b) Recurrent Neural Network\n",
    "- c) Fully Connected Neural Network\n",
    "- d) Both A and B\n",
    "\n",
    "Ans: (D) Both CNN and RNN does weight sharing.\n",
    "\n",
    "## Q3. Which units works as ON and OFF switches in a LSTM?\n",
    "\n",
    "- a) Sigmoid\n",
    "- b) Tanh\n",
    "- c) ReLU\n",
    "- d) None of the above\n",
    "\n",
    "Ans: (A) Sigmoid units used to decide whether the information need to be propagate or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
